---
title: "Worked Solutions to the Exercises"
output:
  html_document:
    toc: no
bibliography: ../sbied.bib
csl: ../jss.csl
---

```{r setup,child="../setup.Rmd"}
```


---------------------------

## Exercise 8.1

A PanelPOMP model with all parameters unit-specific is identical to a collection of independent POMP models. There may not be anything to be gained by using the PanelPOMP framework in this case, except perhaps that the **panelPomp** package can help with the book-keeping for working with the collection of panels.

One can expect an overhead from using **panelPomp** software when the simpler **pomp** software is sufficient.

PanelPOMP models with all unit-specific parameters can be used to test **panelPomp** methodology against **pomp** methodology since the latter also applies in this special case.

-------------

## Exercise 8.2 

```{r echo=FALSE}
library(panelPomp)
```

We can first check the list of help topics:

```{r eval=F}
library(help=panelPomp)
```


```{r echo=F}
xx <- library(help=panelPomp)
cat(paste(c(xx$info[[2]],"\n"),collapse="\n"))
```

And then check out the promising ones:

`?panelPomp_methods`

```{css help_page_style,echo=FALSE}
#help {
	text-align: left;
	border-width: 2px;
	width: 90%;
	height: 50ex;
	padding-left: 10px;
	padding-right: 10px;
	padding-top: 10px;
	padding-bottom: 20px;
}
```
```{r help_page,cache=FALSE,include=FALSE,purl=FALSE}
pgpth <- "help_page.html"
tools::Rd2HTML(
         utils:::.getHelpFile(help(panelPomp_methods,package="panelPomp")),
         stylesheet="https://cran.r-project.org/R.css",
         out=pgpth
        )
```

<iframe id="help" src="`r pgpth`"></iframe>

--------------------------------

## Exercise 8.3

```{r echo=F}
library(panelPomp)
```

* Since a PanelPOMP is a collection of independent POMP models, they can each be filtered separately.

* The `pfilter` method for class `panelPomp` does exactly this, after extracting the parameter vector for each unit from the shared and unit-specific parameters belonging to the `panelPomp` object.

```{r}
contacts <- panelPompExample(pancon)
pf <- pfilter(contacts,Np=100)
class(pf)
class(unitobjects(pf)[[1]])
```

----------------------------

## Exercise 8.4

```{r echo=F}
library(panelPomp)
```

* It turns out that the SMC algorithm implemented by `pfilter()` gives an unbiased Monte Carlo estimate of the likelihood.

* For inferential purposes, we usually work with the log likelihood. 

* Due to Jensen's inequality, SMC has a negative bias as an estimator of the log likelihood, i.e., it systematically underestimates the log likelihood. 

* Usually, the higher the Monte Carlo variance on the likelihood, the larger this bias.

* Thus, lower Monte Carlo variance on the log likelihood is commonly associated with higher estimated log likelihood. 

* Heuristically, products propagate error rapidly. Averaging Monte Carlo replicates over units before taking a product reduces this error propagation. It does not lead to bias in the likelihood estimate since independence over units and Monte Carlo replicates insures that the expected product of the averages is the expected average of the products.

### Some notation for a more formal investigation

* Let $\hat\lambda_u^{(k)}$ be the $k$th replication of the Monte Carlo log likelihood evaluation for unit $u$.

* Let $\hat L_u^{(k)}=\exp\big\{\hat\lambda_u^{(k)}\big\}$ be the corresponding likelihood.

* Let  $\hat\lambda^{(k)}=\sum_{u=1}^U \lambda_{u}^{(k)}$ be an estimate the log likelihood of the entire data based on replication $k$.

* Let $\hat L^{(k)}=\exp\big\{\hat\lambda^{(k)}\big\}$ be the corresponding estimate of the likelihood.

* Different possible estimates of the actual log likelihood $\lambda=\sum_{u=1}^U \lambda_u$ are

\begin{eqnarray} \hat\lambda^{[1]} &=& \frac{1}{K}\sum_{k=1}^K \hat\lambda^{(k)} 
\\
  \hat\lambda^{[2]} &=& \log \left( \frac{1}{K}\sum_{k=1}^K 
  \exp \big\{\hat \lambda^{(k)} \big\} \right)  
\\
 \hat\lambda^{[3]} &=& \sum_{u=1}^U\frac{1}{K}\sum_{k=1}^K \hat\lambda^{(k)}_u  
\\
  \hat\lambda^{[4]} &=& \sum_{u=1}^U \log \left( \frac{1}{K}\sum_{k=1}^K \exp\big\{\hat \lambda^{(k)}_u \big\} \right)  
\end{eqnarray}

(a) Check that $\hat\lambda^{[1]}$ and $\hat\lambda^{[3]}$ are equal. 
However, they are inconsistent, since $\hat\lambda^{(k)}_u$ is a biased estimate of $\lambda_u$ and the bias does not disappear when we take an average over replicates.
(b) $\hat\lambda^{[2]}$ is the log mean exp of the total log likelihood for all units.
(c) $\hat\lambda^{[4]}$ is the sum of the log mean exp for each unit separately. 

* To compare variances, it is convenient to move back to the likelihood scale:

\begin{eqnarray} 
  \hat L^{[2]} &=& \frac{1}{K}\sum_{k=1}^K \prod_{u=1}^U L^{(k)} 
\\
  \hat L^{[4]} &=& \prod_{u=1}^U \frac{1}{K}\sum_{k=1}^K \hat L^{(k)}_u  
\end{eqnarray}


* @breto19 showed that $\hat L^{[4]}$ is smaller than  $\hat L^{[2]}$.

* In a limit where $U$ and $K$ both grow together, $\hat L^{[4]}$ can be stable while $\hat L^{[2]}$ increases to infinity.

-----------

[Back to the lesson](index.html)  

-----------

## References
