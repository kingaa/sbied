


\begin{frame}[fragile]{Do we need both dynamic noise and measurement noise?}

\bi

\item Let's call $H_{\sigma\psi}$ the hypothesis that $\sigma\neq 0$ and $\psi\neq 0$. Write $H_{\sigma\bullet}$ for the nested hypothesis having $\psi=0$, $H_{\bullet\psi}$ having $\sigma=0$, and $H_{\bullet\bullet}$ having both $\sigma=0$ and $\psi=0$.

\ei

\end{frame}

\begin{frame}[fragile]{Dynamic noise only}

\bi

\item We fit $H_{\sigma\bullet}$ using the same algorithmic parameters we used for $H_{\sigma\psi}$.

\ei

<<box_search_global_sigma>>=
params_box_sigma <- params_box[c("Beta","mu_I","rho","sigma"),]
fixed_params_sigma <- c(fixed_params,psi=1e6)

params_box_sigma %>%
  apply(1,function(x)runif(NGLOBAL,x[1],x[2])) %>%
  as.data.frame() -> guesses

mf1 <- mifs_local[[1]]

bake(file="box_search_global_sigma.rds",seed=4397288,kind="L'Ecuyer",{
  foreach(guess=iter(guesses,"row"), 
    .packages='pomp2', 
    .combine=rbind,
    .options.multicore=list(set.seed=TRUE),
    .inorder=FALSE,
    .export=c("mf1","fixed_params_sigma")
  ) %dopar% 
  {
    mf1 %>% mif2(
              params=c(unlist(guess),fixed_params_sigma),
              Nmif=NMIF,  Np=NP_MIF,
              rw.sd=rw.sd(Beta=0.02,mu_I=0.02,rho=0.02,sigma=0.02)
            ) -> mf
    ll <- replicate(N_LIK_REPS,logLik(pfilter(mf,Np=NP_LIK)))
    ll <- logmeanexp(ll,se=TRUE)
    c(coef(mf),loglik=ll[1],loglik=ll[2])
  }
}) -> results_global_sigma
results_global_sigma <- as.data.frame(results_global_sigma)
@

\bi

\item The best result of this search had a likelihood of \Sexpr{round(max(results_global_sigma$loglik),1)} with a standard error of \Sexpr{round(results_global_sigma$loglik.se[which.max(results_global_sigma$loglik)],2)}.

\item To calibrate this, we fit $H_{\bullet\bullet}$.

\ei

<<box_search_global_dot_dot>>=
params_box_dot_dot <- params_box[c("Beta","mu_I","rho"),]
fixed_params_dot_dot <- c(fixed_params,psi=1e6,sigma=1e-6)

params_box_dot_dot %>%
  apply(1,function(x)runif(NGLOBAL,x[1],x[2])) %>%
  as.data.frame() -> guesses

mf1 <- mifs_local[[1]]

bake(file="box_search_global_dot_dot.rds",seed=348876,kind="L'Ecuyer",{
  foreach(guess=iter(guesses,"row"), 
    .packages='pomp2', 
    .combine=rbind,
    .options.multicore=list(set.seed=TRUE),
    .inorder=FALSE,
    .export=c("mf1","fixed_params_dot_dot")
  ) %dopar% 
  {
    mf1 %>% mif2(
              params=c(unlist(guess),fixed_params_dot_dot),
              Nmif=NMIF,  Np=NP_MIF,
              rw.sd=rw.sd(Beta=0.02,mu_I=0.02,rho=0.02)
            ) -> mf
    ll <- replicate(N_LIK_REPS,logLik(pfilter(mf,Np=NP_LIK)))
    ll <- logmeanexp(ll,se=TRUE)
    c(coef(mf),loglik=ll[1],loglik=ll[2])
  }
}) -> results_global_dot_dot
results_global_dot_dot <- as.data.frame(results_global_dot_dot)
@

\bi

\item The best result of this search had a likelihood of \Sexpr{round(max(results_global_dot_dot$loglik),1)} with a standard error of \Sexpr{round(results_global_dot_dot$loglik.se[which.max(results_global_dot_dot$loglik)],2)}.

\item We see that dynamic overdispersion, by itself, gives substantially less improvement in model fit compared to $H_{\sigma\psi}$ (for which we found a maximized likelihood of \Sexpr{round(max(results_global$loglik),1)}).

\ei

\end{frame}

\begin{frame}[fragile]{Measurement overdispersion only}

Now let's try fitting $H_{\bullet\psi}$.
<<box_search_global_psi>>=
params_box_psi <- params_box[c("Beta","mu_I","rho","psi"),]
fixed_params_psi <- c(fixed_params,sigma=1e-6)

params_box_psi %>%
  apply(1,function(x)runif(NGLOBAL,x[1],x[2])) %>%
  as.data.frame() -> guesses

mf1 <- mifs_local[[1]]

bake(file="box_search_global_psi.rds",seed=29127834,kind="L'Ecuyer",{
  foreach(guess=iter(guesses,"row"), 
    .packages='pomp2', 
    .combine=rbind,
    .options.multicore=list(set.seed=TRUE),
    .inorder=FALSE,
    .export=c("mf1","fixed_params_psi")
  ) %dopar% 
  {
    mf1 %>% mif2(
              params=c(unlist(guess),fixed_params_psi),
              Nmif=NMIF, Np=NP_MIF,
              rw.sd=rw.sd(Beta=0.02,mu_I=0.02,rho=0.02,psi=0.02)
            ) -> mf
    ll <- replicate(N_LIK_REPS,logLik(pfilter(mf,Np=NP_LIK)))
    ll <- logmeanexp(ll,se=TRUE)
    c(coef(mf),loglik=ll[1],loglik=ll[2])
  }
}) -> results_global_psi
results_global_psi <- as.data.frame(results_global_psi)
@

\bi

\item The best result of this search had a likelihood of \Sexpr{round(max(results_global_psi$loglik),1)} with a standard error of \Sexpr{round(results_global_psi$loglik.se[which.max(results_global_psi$loglik)],2)}.

\item Measurement overdispersion alone is apparently sufficient to improve the fit over $H_{\bullet\bullet}$. 

\item Note that both $H_{\sigma\bullet}$ and $H_{\bullet\bullet}$ have high uncertainty on their likelihood evaluations for this level of Monte Carlo effort ($J=\Sexpr{NP_LIK}$ particles).

\item The models with $\psi\neq 0$ have much lower uncertainty.

\item If we wanted to pay careful attention to the models with $\psi=0$ we would need many more particles.

\ei

\end{frame}

\begin{frame}[fragile]{\myexercise. Interpretation of filtering error}

\bi

\item It is convenient to work with a model having low filtering error. We can use fewer particles, saving computational effort. Here, the difference is several orders of magnitude. 

\item What (if anything) is the {\it scientific} logic for preferring a model with low filtering error?

\ei

\end{frame}

\begin{frame}[fragile]{Interpreting the comparative results for these models}

\bi

\item To understand what is going on, let's look at the fitted MLE models under these different hypotheses:

\ei

<<MLEs>>=
mle_sigma_psi <- unlist(results_global[which.max(results_global$loglik),paramnames])
mle_dot_psi <- unlist(results_global_psi[which.max(results_global_psi$loglik),paramnames])
mle_sigma_dot <- unlist(results_global_sigma[which.max(results_global_sigma$loglik),paramnames])
mle_dot_dot <- unlist(results_global_dot_dot[which.max(results_global_dot_dot$loglik),paramnames])
cbind(mle_sigma_psi,mle_dot_psi,mle_sigma_dot,mle_dot_dot)
@

\bi

\item We see that a main difference is a lower reporting rate estimate for $H_{\psi\bullet}$.

\item Let's see which data points are primarily responsible for the improvement in the likelihood

\item A useful technique is to plot the difference in conditional log liklihoods between the two hypotheses, for each data point. 

\ei

<<fitted-models>>=
pf_psi <- pfilter(flu,params=mle_dot_psi,Np=5*NP,pred.mean=TRUE)
pf_sigma <- pfilter(flu,params=mle_sigma_dot,Np=5*NP,pred.mean=TRUE)
plot(cond.logLik(pf_psi)-cond.logLik(pf_sigma))
@

\bi

\item We see that the big gain for $H_\psi$ is at the end of the time series.

\item Around the main peak of incidence, $H_\sigma$ actually fits better.

\item To better understand what is going on, let's look at the one-step prediction means. The differences between these and the observations are the residuals.

\ei

<<predictions>>=
plot(obs(flu)["B",])
lines(pred.mean(pf_psi)["R1",]*mle_dot_psi["rho"],lty="dashed",col="blue")
lines(pred.mean(pf_sigma)["R1",]*mle_sigma_dot["rho"],lty="dotted",col="red")

@

\bi

\item We see that the MLE found for $H_{\sigma\bullet}$ cannot explain how the cases drop off so fast, particularly on day 12.

\item The fit for $H_{\bullet\psi}$ may appear superficially worse, but is objectively better by the standards of likelihood.

\ei

\end{frame}

\begin{frame}[fragile]{Conclusions}

\bi

\item We should not be entirely satisfied with any of the models we have considered so far.

\item A better model might be able to capture the peak as well as the early and late epidemic.

\item Perhaps a non-exponential infectious period, or the inclusion of a latent period, could help to explain why the number of cases was able to drop so fast.

\item There is scope for more modeling. Within the POMP framework, we can consider other extensions to the model and assess the consequences.

\item No model is perfect. It is a sign of due diligence that we have found at least some weaknesses in our model.

\item Beyond the observation that there is probably scope for an even better model, what can be gained from developing models such as these?

\item Adequate modeling of stochasticity is critical for the statistical validity of the model, even when the topic is not of direct scientific interest.

\item This is just a toy example on a small dataset. The underlying issue of overdispersion, and its effect on parameter estimates and their confidence intervals, needs consideration in any inference using count data.

\ei

\end{frame}

\begin{frame}[fragile]{A likelihood profile}

\bi

\item The boarding school flu example with measurement overdispersion is very quick to filter. It has only 14 data points, and order 1000 particles are sufficient for SMC filtering.

\item Let's use this as an exercise for constructing likelihood profiles.

\item We may be curious about the lack of convergence points to the left of the maximum for the reporting rate. Is this a result of a sharp cliff in the likelihood surface, or some artifact of the maximization procedure? Computing a profile likelihood over $\rho$ will help us to find out.

\item First, we set up an array of starting points for each profile point.

\item This is similar to the array of random starting values used for the global paramter search, but with \code{rho} varying systematically.

\item The \pkg{pomp} function \code{profileDesign} is useful for constructing this array.

\ei

<<profile_design,eval=F>>=
PROFILE_REPLICATES <- 10 
PROFILE_POINTS <- 10
non_profile_pars <- setdiff(paramnames,"rho")

# theta.t.hi <- partrans(bsflu,c(params_box[,2],params["mu_R1"]),"toEstimationScale")
# theta.t.lo <- partrans(bsflu,c(params_box[,1],params["mu_R1"]),"toEstimationScale")

theta.t.hi <- c(params_box[,2],params["mu_R1"])
theta.t.lo <- c(params_box[,1],params["mu_R1"])

profileDesign(
  rho=seq(from=0.5,to=0.95,length=PROFILE_POINTS),
  lower=theta.t.lo[non_profile_pars],upper=theta.t.hi[non_profile_pars],nprof=PROFILE_REPLICATES
) -> pd
@

\bi

\item Now, we maximize from each of the \Sexpr{PROFILE_REPLICATES} starting values at each of the \Sexpr{PROFILE_POINTS} points along the profile.

\ei

<<compute_profile,eval=F>>=
NMIF_PROFILE <- 50
NP_MIF_PROFILE <- 1000
NP_LIK_PROFILE <- 1000
N_LIK_REPS_PROFILE <- 3

set.seed(8827162,kind="L'Ecuyer") 

mf1 <- mifs_local[[1]]

bake(file="profile_rho.rds",{
  foreach(guess=iter(pd,"row"), 
    .packages='pomp2', 
    .combine=rbind,
    .options.multicore=list(set.seed=TRUE),
    .inorder=FALSE,
    .export=c("mf1","fixed_params_psi")
  ) %dopar% 
  {
    mf1 %>% mif2(
              params=guess,
              Nmif=NMIF_PROFILE,
              Np=NP_MIF_PROFILE,
              rw.sd=rw.sd(Beta=0.02,mu_I=0.02,sigma=0.02,psi=0.02)
            ) -> mf
    ll <- replicate(N_LIK_REPS_PROFILE,logLik(pfilter(mf,Np=NP_MIF_PROFILE)))
    ll <- logmeanexp(ll,se=TRUE)
    c(coef(mf),loglik=ll[1],loglik=ll[2])
  } 
}) -> results_profile_rho
t_profile_rho <- attr(results_profile_rho,"system.time")
results_profile_rho <- as.data.frame(results_profile_rho)
@

\bi

\item This took \Sexpr{round(t_profile_rho["elapsed"]/60,1)} minutes altogether using \Sexpr{n_global} processors.

\item The best result of this search had a likelihood of \Sexpr{round(max(results_profile_rho$loglik),1)} with a standard error of \Sexpr{round(results_profile_rho$loglik.se[which.max(results_profile_rho$loglik)],2)}.

\ei

<<pairs_profile>>=
list(
  guess=pd,
  result=filter(results_profile_rho, loglik > max(loglik)-50)
) %>%
  ldply(.id="type") -> all

pairs(~loglik+Beta+mu_I+rho+sigma+psi, data=all,
  col=ifelse(all$type=="guess", grey(0.5), "red"), pch=16)
@

\bi

\item We may conclude that the profile likelihood for $\rho$ to the left of the MLE is not sharp enough to explain why the original search didn't pay much attention to this region of parameter space.

\item Fitting a 5 parameter model to 14 data points, it is not surprising to find at least one weakly identified parameter. Perhaps it is more suprising that the data can identify some of the parameters fairly accurately.

\ei

\end{frame}
