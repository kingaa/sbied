---
title: "Fitting the SEIR model"
author: "Aaron A. King, Edward L. Ionides, Qianying Lin"
output:
  html_document:
    toc: no
bibliography: ../sbied.bib
csl: ../jss.csl
params:
  prefix: Q_fit_seir/
---

```{r knitr-opts,include=FALSE,purl=FALSE,child="../setup.Rmd",eval=F}
```

## Exercise

Following the template in Lesson 4, estimate the parameters and likelihood of the SEIR model you implemented in the earlier lessons.
Specifically:

(a) First conduct a local search and then a global search using the multi-stage, multi-start method displayed above.

(b) How does the maximized likelihood compare with what we obtained for the SIR model?

(c) How do the parameter estimates differ?

You will need to tailor the intensity of your search to the computational resources at your disposal.
In particular, choose the number of starts, number of particles employed, and the number of IF2 iterations to perform in view of the size and speed of your machine.

-----------------

## Solution 

We start by building a pomp object combining the SEIR process model from [Exercise 2.4](../stochsim/exercises.html\#basic-exercise-the-seir-model) with the negative binomial measurement model used in Lessons 3 and 4.

```{r seir_pomp_libs,echo=T,results="hide",message=F,warning=F}
library(pomp)
library(tidyverse)
library(doParallel)
library(doRNG)
```

```{r sir_model,echo=T,results="hide",message=F,warning=F}
source("https://kingaa.github.io/sbied/pfilter/model.R")
```

```{r seir_pomp}
seir_step <- Csnippet("
  double dN_SE = rbinom(S,1-exp(-Beta*I/N*dt));
  double dN_EI = rbinom(E,1-exp(-mu_EI*dt));
  double dN_IR = rbinom(I,1-exp(-mu_IR*dt));
  S -= dN_SE;
  E += dN_SE - dN_EI;
  I += dN_EI - dN_IR;
  R += dN_IR;
  H += dN_IR;
")

seir_rinit <- Csnippet("
  S = nearbyint(eta*N);
  E = 0;
  I = 1;
  R = nearbyint((1-eta)*N);
  H = 0;
")

measSEIR <- pomp(measSIR,
  rprocess=euler(seir_step,delta.t=1/7),
  rinit=seir_rinit,
  paramnames=c("N","Beta","mu_EI","mu_IR","eta","k","rho"),
  partrans=parameter_trans(
        log=c("Beta","mu_EI","mu_IR","k"),
        logit=c("eta","rho")
  ),
  statenames=c("S","E","I","R","H")
)

coef(measSEIR) <- c(coef(measSIR),mu_EI=0.8)
fixed_params <- c(N=38000, mu_IR=2, k=10)
coef(measSEIR,names(fixed_params)) <- fixed_params
```

To debug the model and provide a sanity check on our parameter guesses, we first explore via simulation. Some simulations die out, but others lead to epidemics somewhat resembling the data.
```{r simulate}
set.seed(1)
plot(simulate(measSEIR))
```

The next prerequisite is that we can successfully filter:
```{r pfilter}
pf1 <- pfilter(measSEIR,1000)
plot(pf1)
logLik(pf1)
```
The minimum effective sample size is `r round(min(pf1@eff.sample.size),0)`, which is not a complete disaster, and we should bear in mind that this is likely to improve when we fit the parameters.

We now carry out a local search, estimating only 4 parameters for simplicity. For a thorough scientific analysis, one would also want to consider the evidence in the data concerning the other parameters that are fixed here.
```{r local_search}
mycores <- detectCores()
registerDoParallel(mycores)
registerDoRNG(482947940)
bake(file="Q_fit_seir_local_search.rds",{
  foreach(i=1:20,.combine=c) %dopar% {
    library(pomp)
    library(tidyverse)
    measSEIR %>%
      mif2(
        Np=2000, Nmif=50,
        cooling.fraction.50=0.5,
        rw.sd=rw.sd(Beta=0.02, rho=0.02, eta=ivp(0.02),mu_EI=0.02)
      )
  }
}) -> mifs_local
```

This consistently obtains log likelihoods around -104, similar to those found with the SIR model:
```{r}
sapply(mifs_local,logLik)
```

As usual, we should evaluate the likelihoods using a particle filter, rather than relying on the likelihood from the last filtering iteration of the perturbed model used by `mif2`.
```{r local_search_evaluations}
registerDoRNG(900242057)
bake(file="Q_fit_seir_lik_local.rds",{
  foreach(mf=mifs_local,.combine=rbind) %dopar% {
    library(pomp)
    library(tidyverse)
    evals <- replicate(10, logLik(pfilter(mf,Np=5000)))
    ll <- logmeanexp(evals,se=TRUE)
    mf %>% coef() %>% bind_rows() %>%
      bind_cols(loglik=ll[1],loglik.se=ll[2])
  }
}) -> local_logliks
```

In this case, there is not much discrepancy between the perturbed and unperturbed likelihoods. The small improvement (rather than disadvantage) from filtering with fixed parameters supports a hypothesis that the constant parameter model is reasonable here.
```{r}
local_logliks$loglik
```

```{r cluster_setup}
  if (file.exists("CLUSTER.R")) {
    source("CLUSTER.R")
  }
```
  
```{r global_search_design}
  set.seed(2062379496)

  runif_design(
    lower=c(Beta=5,rho=0.2,eta=0,mu_EI=1/3),
    upper=c(Beta=80,rho=0.9,eta=1,mu_EI=3),
    nseq=100
  ) -> guesses
  mf1 <- mifs_local[[1]]
```
  
```{r global_search}
bake(file="Q_fit_seir_global_search.rds",{
  registerDoRNG(1270401374)
  foreach(guess=iter(guesses,"row"), .combine=rbind) %dopar% {
    library(pomp)
    library(tidyverse)
    mf1 %>%
      mif2(params=c(unlist(guess),fixed_params),Np=1000) %>%
      mif2(Nmif=100) -> mf
    replicate(
      10,
      mf %>% pfilter(Np=2000) %>% logLik()
    ) %>%
      logmeanexp(se=TRUE) -> ll
    mf %>% coef() %>% bind_rows() %>%
      bind_cols(loglik=ll[1],loglik.se=ll[2])
  }
}) -> results
```

The maximum log likelihood discovered is `r round(max(results$loglik),1)`.
This small improvement of around one log unit is not compelling evidence by itself for the need of an extra parameter.
However, the interpretation of the fitted model has some interesting features, which can be seen from a scatter plot.

```{r scatter_plot}
pairs(~loglik+Beta+eta+rho+mu_EI,data=filter(results,loglik>max(loglik)-10))
```

When including an latent period, the MLE has intermediate values of $\rho$ and $\eta$ that match epidemiological expectations for endemic measles in the pre-vaccine era, while remaining consistent with a mean infectious period of 0.5 wk.
This is substantially different from the results in Section 5 of Lesson 4.  
Thus, adding a latent period to the model can substantially change the interpretation of the fitted model without substantially changing the overall fit measured by maximized likelihood.
Profile likelihood calculations could help to clarify this finding.

The likelihood surface here is fairly flat: the y-axis range is just 2 log units.
Data on a single epidemic cannot readily distinguish whether the disease has a high susceptible fraction and low reporting rate, or low susceptible fraction and high reporting rate.
Longer time series could resolve this question.

-----------------------------------

[Licensed under the Creative Commons Attribution-NonCommercial license](http://creativecommons.org/licenses/by-nc/4.0/).
Please share and remix noncommercially, mentioning its origin.  
![CC-BY_NC](../graphics/cc-by-nc.png)

[**Back to the lesson**](./index.html)  
[**Course homepage**](../index.html)  
