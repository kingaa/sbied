\input{header}
\input{../_includes/quiz_header}

\title{Quiz}
\begin{document}

% knitr set up
<<knitr_opts,include=FALSE,cache=FALSE,purl=FALSE>>=
source("../_includes/setup.R", local = knitr::knit_global())
@

<<prelims,echo=F,cache=F>>=
library(tidyverse)
library(pomp)
options(stringsAsFactors=FALSE)
set.seed(1350254336)
knitr::opts_chunk$set(highlight=FALSE)
@

\maketitle

\section{}
Suppose that 10 replications of a particle filter, each using $10^3 $ particles, runs in 15 minutes with no parallelization. To look for a more precise likelihood evaulation, you consider running 20 replicates, each with $10^4$ particles. How many minutes will this take, if you distribute the calculation across 4 cores?
\begin{enumerate}[(A)]
\item \label{A2a} 50
\item \label{A2b} 60
\item \label{A2c} 75
\item \label{A2d} 120
\item \label{A2e} 300
\end{enumerate}

\begin{solution}
  \ref{A2c}. Using the linear dependence, also called proportionality, of the computing effort on various algorithmic parameters, we calculate
  $$5\times (10000/1000)\times (20/10)\times (1/4)=75.$$
\end{solution}

\section{}
A particle filter is repeated 5 times to evaluate the likelihood at a proposed maximum likelihood estimate, each time with $10^4$ particles. Suppose the log likelihood estimates are $-2446.0$, $-2444.0$, $-2443.0$, $-2442.0$, $-2440.0$. Which of the following is an appropriate estimate for the log likelihood at this parameter value and its standard error.
\begin{enumerate}[(A)]
\item \label{A6a} Estimate $= -2443.0$, with standard error 1.0
\item \label{A6b} Estimate $= -2443.0$, with standard error 2.2
\item \label{A6c} Estimate $= -2443.0$, with standard error 5.0
\item \label{A6d} Estimate $= -2441.4$, with standard error 2.2
\item \label{A6e} Estimate $= -2441.4$, with standard error 1.4
\end{enumerate}

\begin{solution}
  \ref{A6e}.
  Answers \ref{A6a}, \ref{A6b} and \ref{A6c} estimate using a mean on the log scale. However, the particle filter provides an unbiased likelihood estimate on a natural scale but not on a log scale. Note that the particle filter also has some bias for most quantities on a natural scale, which reduces to zero as the number of particles tends to infinity, but it happens to be unbiased for the likelihood. The standard error for the log of the mean of the likelihoods can be computed by the delta method or a jack-knife, for example using the logmeanexp function in pomp.

  <<Q6>>=
  ll <- c(-2446,-2444,-2443,-2442,-2440)
  mean(ll)
  sd(ll)
  sd(ll)/sqrt(length(ll))
  library(pomp)
  logmeanexp(ll,se=TRUE)
  @
\end{solution}


\section{}
What is the log likelihood (to the nearest unit) of the Dacca cholera data for the POMP model constructed in pomp via
<<ebolaModel,echo=TRUE,eval=FALSE>>=
d <- dacca(deltaI=0.08)
@
with cholera mortality rate 8\% per year, and other parameters fixed at the default values.
 
\begin{enumerate}[(A)]
\item \label{A9a} -3764
\item \label{A9b} -3765
\item \label{A9c} -3766
\item \label{A9d} -3767
\item \label{A9e} -3768
\end{enumerate}

\begin{solution}
  \ref{A9a}, calculated as follows:

  <<Q9>>=
  d <- dacca(deltaI=0.08)
  library(doFuture)
  plan(multisession)

  bake(file="Q9.rds",{
    foreach(i=1:32,.combine=c,
      .options.future=list(seed=TRUE)
    ) %dofuture% {
      library(pomp)
      logLik(pfilter(d,Np=10000))
    }
  }) -> cholera_loglik
  logmeanexp(cholera_loglik,se=TRUE)
  @
\end{solution}


\section{}
Effective sample size (ESS) is one of the main tools for diagnosing the success of a particle filter.
If you plot an object of class \code{pfilterd\_pomp} (created by applying \code{pfilter} to a pomp object), the ESS is displayed.
Suppose one or more time points have low ESS (say, less than 10) even when using a fairly large number of particles (say, $10^4$).
What is the proper interpretation?

\begin{enumerate}[(A)]
\item \label{A14a} There is a problem with data, perhaps an error recording an observation.
\item \label{A14b} There is a problem with the model which means that it cannot explain something in the data.
\item \label{A14c} The model and data have no major problems, but the model happens to be problematic for the particle filter algorithm.
\item \label{A14d} At least one of \ref{A14a}, \ref{A14b} and \ref{A14c}.
\item \label{A14e} Either \ref{A14a} or \ref{A14b} or both, but not \ref{A14c}. If the model fits the data well, the particle filter is guaranteed to work well. 
\end{enumerate}

\begin{solution}
  \ref{A14d}.
  An example of a situation where the model fits the data well, but filtering is hard, arises when the measurement error is small relative to the process noise. In this case, the particles are scattered by the process noise and very few of them are compatible with the data due to the precise measurement. Thus, almost all the particles must be discarded as unfeasible given the data, corresponding to a low ESS.
\end{solution}

\end{document}
