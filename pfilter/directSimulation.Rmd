---
title: "Likelihood by direct simulation: Consett measles example"
author: "Aaron A. King and Edward L. Ionides"
output:
  html_document:
    toc: yes
    toc_depth: 4
    includes:
      after_body:
      - ../_includes/supp_bottom.html
      - ../_includes/license.html
bibliography: ../sbied.bib
csl: ../jss.csl
params:
  prefix: direct
---

\newcommand\prob[1]{\mathbb{P}\left[{#1}\right]}
\newcommand\expect[1]{\mathbb{E}\left[{#1}\right]}
\newcommand\var[1]{\mathrm{Var}\left[{#1}\right]}
\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dd[1]{d{#1}}
\newcommand\lik{\mathcal{L}}
\newcommand\loglik{\ell}

-----------------------------------

```{r knitr-opts,include=FALSE,purl=FALSE}
source("../setup.R", local = knitr::knit_global())
```
```{r prelims,include=FALSE,cache=FALSE}
library(tidyverse)
library(pomp)
set.seed(594709947L)
```

We're going to demonstrate what happens when we attempt to compute the likelihood for the Consett measles outbreak data by direct simulation from.

First, let's reconstruct the toy SIR model we were working with.

```{r model-construct}
source("https://kingaa.github.io/sbied/pfilter/model.R")
```

Let's generate a large number of simulated trajectories at some particular point in parameter space.
```{r bbs-mc-like-2}
measSIR %>%
  simulate(nsim=5000,format="arrays") -> x
sims <- coef(measSIR,"rho")*x$states["H",,]
matplot(time(measSIR),t(sims[1:50,]),type='l',lty=1,
  xlab="time",ylab=expression(rho*H),bty='l',col='blue')
lines(time(measSIR),obs(measSIR,"reports"),lwd=2,col='black')
```

We can use the function `dmeasure` to evaluate the log likelihood of the data given the states, the model, and the parameters:
```{r bbs-mc-like-3,cache=TRUE}
ell <- dmeasure(measSIR,y=obs(measSIR),x=x$states,times=time(measSIR),log=TRUE,
  params=coef(measSIR))
dim(ell)
```
According to the general equation for likelihood by direct simulation, we should sum up the log likelihoods across time:
```{r bbs-mc-like-4}
ell <- apply(ell,1,sum)
summary(ell)
summary(exp(ell))
```

- The variability in the individual likelihoods is high and therefore the likelihood esitmate is imprecise.
We will need many simulations to get an estimate of the likelihood sufficiently precise to be of any use in parameter estimation or model selection.

- What is the problem?

	- Essentially, very few of the trajectories pass anywhere near the data and therefore almost all have extremely bad likelihoods.
Moreover, once a trajectory diverges from the data, it almost never comes back.
While the calculation is "correct" in that it will converge to the true likelihood as the number of simulations tends to $\infty$, we waste a lot of effort investigating trajectories of very low likelihood.

	- This is a consequence of the fact that we are proposing trajectories in a way that is completely unconditional on the data.

- The problem will get much worse with longer data sets.

-----------------------------------

Produced in R version `r getRversion()`.

--------------------------
