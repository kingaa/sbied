---
title: "Monte Carlo Methods"
author: "Aaron A. King and Edward L. Ionides"
output:
  html_document:
    toc: yes
    toc_depth: 4
bibliography: ../sbied.bib
csl: ../ecology.csl
---

\newcommand\prob{\mathbb{P}}
\newcommand\E{\mathbb{E}}
\newcommand\var{\mathrm{Var}}
\newcommand\cov{\mathrm{Cov}}
\newcommand\loglik{\ell}
\newcommand\R{\mathbb{R}}
\newcommand\data[1]{#1^*}
\newcommand\params{\, ; \,}
\newcommand\transpose{\scriptsize{T}}
\newcommand\eqspace{\quad\quad}
\newcommand\myeq[1]{\eqspace \displaystyle #1}
\newcommand\lik{\mathscr{L}}
\newcommand\profileloglik[1]{\ell^\mathrm{profile}_#1}
\newcommand\given{{\, | \,}}
\newcommand\equals{{=\,}}
\newcommand\nmc{j}
\newcommand\Nmc{J}
\newcommand\dimX{\mathrm{dim}(X)}
\newcommand\dimY{\mathrm{dim}(Y)}

-----------------------------------

[Licensed under the Creative Commons Attribution-NonCommercial license](http://creativecommons.org/licenses/by-nc/4.0/).
Please share and remix noncommercially, mentioning its origin.  
![CC-BY_NC](../graphics/cc-by-nc.png)

Produced with **R** version `r getRversion()`.

-----------------------------------

```{r knitr-opts,include=FALSE,purl=FALSE}
library(knitr)
prefix <- "monteCarlo"
opts_chunk$set(
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=TRUE,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",prefix,"-"),
  cache.path=paste0("cache/",prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
  )
```
```{r prelims,include=FALSE,cache=FALSE}
options(
  keep.source=TRUE,
  stringsAsFactors=FALSE,
  encoding="UTF-8"
  )

set.seed(594709947L)
require(ggplot2)
theme_set(theme_bw())
require(plyr)
require(reshape2)
require(magrittr)
require(pomp)
stopifnot(packageVersion("pomp")>="1.18")
```

## Objectives

1. To review some basic ideas in Monte Carlo computation and simulating random variables.

2. To provide a basic introduction to the Monte Carlo approach, and the generation of simulated random variables, for those who haven't seen it before.

<br>

-----------

## Our context: Monte Carlo methods for POMP models.

Let's consider a general POMP model.
As before, let $\data{y_{1:N}}$ be the data, and let the model consist of a latent process $X_{0:N}$ and an observable process $Y_{1:N}$.
Then the likelihood function is
$$\begin{eqnarray}
\lik(\theta)
&=&
f_{Y_{1:N}}(\data{y_{1:N}}\params\theta)
\\
&=&\int_{x_0}\cdots\int_{x_N}\! f_{X_0}(x_0\params\theta)\prod_{n=1}^{N}\!f_{Y_n|X_n}(\data{y_n}\given x_n\params \theta)\, f_{X_n|X_{n-1}}(x_n|x_{n-1}\params\theta)\, dx_0\dots dx_N.
\end{eqnarray}$$
i.e., computation of the likelihood requires integrating (or summing, for a discrete model) over all possible values of the unobserved latent process at each time point.
This is very hard to do, in general.

Let's review, and/or learn, some *Monte Carlo* approaches for evaluating this and other difficult integrals.
An excellent technical reference on Monte Carlo techniques is @Robert2004.

<br>

-----------

## The fundamental theorem of Monte Carlo integration

* The basic insight of Monte Carlo methods is that we can get a numerical approximation to a challenging integral,
$$ H = \int h(x)\, f(x)\, dx,$$
if we can simulate (i.e., generate random draws) from the distribution with probability density function $f$.

* This insight is known as the *fundamental theorem of Monte Carlo integration*.


------

<big><big><b>Theorem</b></big></big>. Let $f(x)$ be the probability distribution function for a random variable $X$, and let $X_{1:J}=\{X_\nmc, \nmc=1,\dots,\Nmc\}$ be an independent and identically distributed sample of size $\Nmc$ from $f$. 
Let ${H_\Nmc}$ be the sample average of $h(X_1)\dots,h(X_\Nmc)$,
$${H_\Nmc} = \frac{1}{\Nmc}\,\sum_{\nmc=1}^{\Nmc}\!h(X_\nmc).$$
Then ${H_\Nmc}$ converges to $H$ as $\Nmc\to\infty$ with probability 1.
Less formally, we write
$${H_\Nmc} \approx \int\!h(x)\,f(x)\,dx.$$

--------

<big><big><b>Proof</b></big></big>. This is the strong law of large numbers, together with the identity that 
$$\E[h(X)] =  \int\!h(x)\,f(x)\,dx.$$

-----------


* We can estimate the error in this approximation, because the empirical variance
$$V_\Nmc = \frac{1}{\Nmc-1}\,\sum_{\nmc=1}^{\Nmc}\!\big[h(X_\nmc)-H_\Nmc\big]^2$$
approximates the true variance, $\var[h(X)]=\E\big[\big(h(X)-\E[h(X)]\big)^2\big]$.

* The standard error on the approximation $H_{\Nmc}\approx \E[h(X)]$ is therefore
$$\sqrt{\frac{V_\Nmc}{\Nmc}}.$$

* From the central limit theorem, the error is approximately normally distributed:
$$H_\Nmc -\E[h(X)]\;\sim\;\mathrm{normal}\left(0,\frac{V_\Nmc}{\Nmc}\right).$$

* The fundamental theorem of Monte Carlo inspires us to give further thought on how to simulate from a desired density function $f$, which may itself be a challenging problem.

* We will review simulation, but first let's consider a useful generalization of the fundamental theorem.

<br>

------------

## Importance sampling

* Sometimes it is difficult to sample directly from the distribution of $X$.

* In this case, we can often make use of *importance sampling*, in which we generate random samples from another distribution (easier to simulate) and make the appropriate correction.

* Specifically, suppose we wish to compute $\mathbb{E}[h(X)]$, where $X\sim{f}$, but it is  difficult or impossible to draw random samples from $f$.

* Suppose $g$ is a probability distribution from which it's relatively easy to draw samples and let $Y_{1:\Nmc}$ be i.i.d. random variables drawn from $g$.

* Notice that
$$\mathbb{E}[h(X)] = \int\!h(x)\,f(x)\,\mathrm{d}x = \int\!h(x)\,\frac{f(x)}{g(x)}\,g(x)\, dx.$$

* So, we can generalize the Monte Carlo integration theorem to give the **Monte Carlo importance sampling theorem**,
$$\mathbb{E}[h(X)] \approx \frac{1}{\Nmc}\,\sum_{\nmc=1}^{\Nmc}\!h(Y_\nmc)\,\frac{f(Y_\nmc)}{g(Y_\nmc)}.$$


* We call $w_\nmc=f(Y_\nmc)/g(Y_\nmc)$ the **importance weights**, and then we can write
$$\mathbb{E}[h(X)] \approx \frac{1}{\Nmc}\,\sum_{\nmc=1}^{\Nmc} w_\nmc \, h(Y_\nmc).$$

* Since $\E[w_\nmc] = \E[f(Y)/g(Y)]=1$, we can modify this formula to give a **self-normalized importance sampling** estimate,
$$\mathbb{E}[h(X)] \approx \frac{\sum\!w_\nmc\,h(Y_\nmc)}{\sum\!w_\nmc}.$$

* The self-normalized estimate requires computation of $w_\nmc$ only up to a constant of proportionality.

* The Monte Carlo variance associated with this estimate is
$$\frac{\sum\!w_\nmc\,(h(Y_\nmc)-\overline{h})^2}{\sum\!w_\nmc}.$$

* Obtaining accurate estimates requires some thought to the importance distribution $g$.
Specifically, if the tails of $g$ are lighter than those of $f$, the Monte Carlo variance will be inflated and the estimates can be unusable.

<br>

-----------

## Simulation techniques for general distributions

* Simulation refers to the generation of random variables.

* The general problem of simulation is: given a probability distribution $f$, find a procedure that generates random draws from $f$.

* This is a very important problem in scientific computing and much thought and effort has gone into producing reliable simulators for many basic random variables.

* There are two basic ways of solving this problem: 

    1. The transformation method,

    2. The rejection method.


<br>

-----------

### The transformation method

* This method works for discrete or continuous scalar random variables.

* Let $f$ be the probability distribution function we seek to draw from (known as the **target distribution**) and $F$ be the corresponding cumulative distribution function, i.e., $F(x) = \int_{-\infty}^x f(v)\, dv$.

* Let $F^{-1}(u) = \inf\{x: F(x)\,\ge\,u\}$ be the inverse of $F$.

* A basic fact is that, if $X\!\sim\!f$, then $F(X)\!\sim\!\mathrm{uniform}(0,1)$.
<br>
Proof: If $f(X)>0$, then 
$$\begin{eqnarray}
\prob[F(X)\,\le\,u] &=& \prob[X\,<\,F^{-1}(u)] 
\\
&=& F\big(F^{-1}(u)\big) = u.
\end{eqnarray}$$

* This suggests that, if we can compute $F^{-1}$, we use the following algorithm to generate $X\!\sim\!f$:

1. draw $U \sim \mathrm{uniform}(0,1)$.

2. let $X = F^{-1}(U)$.

<br>

-----------


### The rejection method

* The transformation method is very efficient in that we are guaranteed to obtain a valid $X$ from the density $f$ for every $U \sim \mathrm{uniform}(0,1)$ we generate.

* Sometimes, however, we cannot compute the inverse of the cumulative distribution function, as required by the transformation method.

* Under such circumstances, the rejection method offers a less efficient, but more flexible, alternative.

* We'll see how and why this method works.

-----------------------------------

#### The rejection method for uniform random variables on arbitrary sets

* Let a random variable $X$ take values in $\R^{\dimX}$.

* Suppose that $X$ is **uniformly distributed** over a region $D\subset \R^{\dimX}$.
This means that, for any ${A}\subset{D}$, 
$$\prob[X\in{A]}=\frac{\mathrm{area}(A)}{\mathrm{area}(D)}.$$
We write
$$X \sim \mathrm{uniform}(D).$$

* Let's suppose that we wish to simulate $X\!\sim\!\mathrm{uniform}(D)$.

* Suppose that we don't know how to directly simulate a random draw from $D$, but we know $D$ is a subset of some nicer region $U\subset \R^{\dimX}$.

* If we know how to generate $Y\!\sim\!\mathrm{uniform}(U)$, then we can simply do so until ${Y}\in{D}$, at which point we take $X=Y$.

* Since for any $A\subset{D}$, 
$$\begin{eqnarray}
\prob[X\in A] &=& \prob[Y\in A |Y\in D]
\\ 
&=& \frac{\mathrm{area}(A)}{\mathrm{area}(U)}\Big{/}\frac{\mathrm{area}(D)}{\mathrm{area}(U)} 
\\
&=& \frac{\mathrm{area}(A)}{\mathrm{area}(D)},
\end{eqnarray}$$
it follows that $Y\!\sim\!\mathrm{uniform}(D)$.

* Consider an analogy to throwing darts. If the darts are thrown in such a way as to be equally likely to land anywhere in $U$, then those that do land in $D$ are equally likely to land anywhere in $D$.


```{r region-diagram,echo=F,,fig.width=2,fig.height=2}
op <- par(mar=c(0,1,0,1),mgp=c(2,1,0),font=4,family="sans")
plot(c(0,1),c(0,1),type='n',ann=F,bty='o',tcl=0)
t <- seq(0,1,by=0.001)
xyc <- rbind(a=c(0.89,0.64),aa=c(0.63,0.81),b=c(0.67,0.96),c=c(0.21,0.89),d=c(0.35,0.35),
             e=c(0.03,0.4),f=c(0.28,0.04),g=c(0.75,0.03),h=c(0.6,0.6))
basis <- periodic.bspline.basis(t,degree=2,nbasis=nrow(xyc),period=1)
xy <- basis%*%xyc
lines(xy[,1],xy[,2],lwd=1.5)
xyc <- rbind(a=c(0.37,0.33),b=c(0.51,0.33),c=c(0.51,0.23),d=c(0.37,0.23))
basis <- periodic.bspline.basis(t,degree=1,nbasis=nrow(xyc),period=1)
xy <- basis%*%xyc
lines(xy[,1],xy[,2])
text(x=c(0.05,0.5,0.9),y=c(0.95,0.75,0.1),labels=c("U","D","A"))
arrows(0.88,0.1,0.44,0.28,lwd=1.5,length=0.1)
par(op)
```    



<br>

--------------

#### The rejection method for dominated densities

* A useful little fact allows us to extend the rejection method from uniform distributions to arbitrary densities. 

* @Robert2004 refer to this fact the *fundamental theorem of simulation*.


--------------


* Let $h$ be an arbitary positive, integrable function. 

* Define
$$D=\{(x,u): 0{\le}u{\le}h(x)\},$$ 
i.e., $D$ is the graph of $h$.

* Consider the random pair $(X,U)\!\sim\!\mathrm{uniform}(D)$.

* What is the marginal distribution of $X$?
$$\int_0^{h(x)}\!\mathrm{d}u = h(x)$$

* So $h$ is the probability distribution function for $X$!

* To carry out the rejection method, simulating from $g$ to obtain a sample from $f$, we take our region $D$ to be the **graph** of $Mg$, i.e.,
$$ D = \{(x,y): 0\le y\le Mg(x),$$
where $M$ is a constant chosen so that $Mg(x)\le f(x)$ for all $x$.

* We propose points $(X,Y)$ by drawing them uniformly from the area under the graph of $M\,g$.

* We **accept** the point $(X,Y)$ if it lies under the graph of $f$.

* Then, the $X$-component of the $(X,Y)$ pair is distributed according to $f$.

------------


* This suggests the following rejection method for simulating an arbitrary random variable.

* Let $f$ be the target distribution and $g$ be another distribution function (from which it is easy to simulate) (see Figure below).

* Let $M$ be such that $M\,g(x){\ge}f(x)$ for all $x$.

* The following procedure simulates $X\!\sim\!f$.

1. draw $Y\!\sim\!g$ and $U\!\sim\!\mathrm{uniform}(0,M\,g(Y))$.

2. if $U{\le}f(Y)$, then let $X=Y$ else repeat step 1.

```{r rejection-method-diagram,echo=F,fig.width=2,fig.height=2}
op <- par(mar=c(0,1,0,1),mgp=c(2,1,0),font=4,family="sans")
x <- seq(-5,10,by=0.01)
f <- 0.2*dnorm(x,mean=-2,sd=0.5)+0.5*dnorm(x,mean=1,sd=1)+0.3*dnorm(x,mean=6,sd=2)
g <- dnorm(x,mean=1,sd=5)
g <- 1.1*max(f)/max(g)*g
xx <- c(6.1,2.7)
yy <- c(0.16,0.12)
plot(x,g,type='l',col='red',xlab='x',ylab="pdf",ylim=c(0,1.05*max(g)),bty='l')
lines(x,f,col='black')
text(xx,yy,labels=c("M g","f"),col=c("red","black"),font=3)
par(op)
```    

-----------------------

## [Back](./pfilter.html)

--------------------------

## References
