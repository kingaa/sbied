---
title: "Monte Carlo Methods"
author: "Aaron A. King"
date: "2015-07-10"
output:
  html_document:
    toc: yes
bibliography: ../sbied.bib
csl: ../ecology.csl
---

\newcommand\prob[1]{\mathbb{P}\left[{#1}\right]}
\newcommand\expect[1]{\mathbb{E}\left[{#1}\right]}
\newcommand\var[1]{\mathrm{Var}\left[{#1}\right]}
\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dlta[1]{{\Delta}{#1}}

-----------------------------------

Licensed under the Creative Commons attribution-noncommercial license, http://creativecommons.org/licenses/by-nc/3.0/.
Please share and remix noncommercially, mentioning its origin.  
![CC-BY_NC](../graphics/cc-by-nc.png)

Produced in R version `r getRversion()`.

-----------------------------------

```{r knitr-opts,include=FALSE,purl=FALSE}
library(knitr)
prefix <- "monteCarlo"
opts_chunk$set(
  progress=TRUE,
  prompt=FALSE,tidy=FALSE,highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  message=FALSE,
  error=FALSE,
  echo=TRUE,
  cache=TRUE,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",prefix,"-"),
  cache.path=paste0("cache/",prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
  )
```
```{r prelims,include=FALSE,cache=FALSE}
options(
  keep.source=TRUE,
  stringsAsFactors=FALSE,
  encoding="UTF-8"
  )

set.seed(594709947L)
require(ggplot2)
theme_set(theme_bw())
require(plyr)
require(reshape2)
require(magrittr)
require(pomp)
stopifnot(packageVersion("pomp")>="0.69-1")
```

## Monte Carlo methods

Let's return to considering a general state-space model.
As before, let $y_1, \dots, y_T$ be the data, $x_1, \dots, x_T$ be the states.
Then the likelihood function is
$$\lik(\theta)=\mathrm{Prob}[y_{1:T}|\theta]=\sum_{x_1}\cdots\sum_{x_T}\!\prod_{t=1}^{T}\!\mathrm{Prob}[y_t|x_t,\theta]\,\mathrm{Prob}[x_t|x_{t-1},\theta]$$

i.e., computation of the likelihood requires summing over all possible values of the unobserved process at each time point.
This is very hard to do, in general.
Today, we will learn about *Monte Carlo* methods for evaluating this and other difficult integrals.
An excellent technical reference on Monte Carlo techniques generally is @Robert2004.

## Simulation

Simulation refers to the generation of random variables.
The general problem of simulation is: given a probability distribution $f$, find a procedure that generates random draws from $f$.
This is a very important problem in scientific computing and much thought and effort has gone into producing reliable simulators for many basic random variables.
There are two basic ways of solving this problem: 

1. the transformation method and
2. the rejection method


### The transformation method

This method works for discrete or continuous scalar random variables.
Let $f$ be the probability distribution function we seek to draw from (the *target distribution*) and $F$ be the cumulative distribution function, i.e., $F(x) = \int^x\!f(x)\,\mathrm{d}x$.
Let $F^{-1}(u) = \inf\{x: F(x)\,\ge\,u\}$ be the inverse of $F$.
A basic fact is that, if $X\!\sim\!f$, then $F(X)\!\sim\!\mathrm{uniform}(0,1)$.
[Proof: If $f(X)>0$, $\mathrm{Prob}[F(X)\,\le\,u] = \mathrm{Prob}[X\,<\,F^{-1](u)} = F(F^{-1}(u)) = u$.]
This suggests that, if we can compute $F^{-1}$, we use the following algorithm to generate $X\!\sim\!f$:

1. draw $U\!\sim\!\mathrm{uniform}(0,1)$.
2. let $X = F^{-1}(U)$.


### The rejection method

The transformation method is very efficient in that we are guaranteed to obtain a valid $X\!\sim\!f$ for every $U\!\sim\!\mathrm{uniform}(0,1)$ we generate.
Sometimes, however, we cannot compute the inverse of the c.d.f., as required by the transformation method.
Under such circumstances, the rejection method offers a less efficient, but more flexible, alternative.
We'll see how and why this method works.

-----------------------------------

Region $D$ lies within region $U$.
A random variable $X$ is said to be uniformly distributed over a region $D$ ($X\!\sim\!\mathrm{uniform}(D)$) if, for any ${A}\subset{D}$, $\mathrm{Prob}[X\in{A]}=\mathrm{area}(A)/\mathrm{area}(D)$.

```{r region-diagram,echo=F}
op <- par(mar=c(0,1,0,1),mgp=c(2,1,0),font=4,family="sans")
plot(c(0,1),c(0,1),type='n',ann=F,bty='o',tcl=0)
t <- seq(0,1,by=0.001)
xyc <- rbind(a=c(0.89,0.64),aa=c(0.63,0.81),b=c(0.67,0.96),c=c(0.21,0.89),d=c(0.35,0.35),
             e=c(0.03,0.4),f=c(0.28,0.04),g=c(0.75,0.03),h=c(0.6,0.6))
basis <- periodic.bspline.basis(t,degree=2,nbasis=nrow(xyc),period=1)
xy <- basis%*%xyc
lines(xy[,1],xy[,2],lwd=1.5)
xyc <- rbind(a=c(0.37,0.33),b=c(0.51,0.33),c=c(0.51,0.23),d=c(0.37,0.23))
basis <- periodic.bspline.basis(t,degree=1,nbasis=nrow(xyc),period=1)
xy <- basis%*%xyc
lines(xy[,1],xy[,2],lwd=1.5)
text(x=c(0.05,0.5,0.9),y=c(0.95,0.75,0.1),labels=c("U","D","A"),cex=1.5)
arrows(0.88,0.1,0.44,0.28,lwd=1.5,length=0.1)
par(op)
```    

-----------------------------------


Diagram of the rejection method.
We propose points by drawing them uniformly from the area under the graph of $M\,g$ and accept them if they lie under the graph of $f$.
The $X$-coordinate of the points are then distributed according to $f$.

```{r rejection-method-diagram,echo=F}
op <- par(mar=c(0,1,0,1),mgp=c(2,1,0),font=4,family="sans")
x <- seq(-5,10,by=0.01)
f <- 0.2*dnorm(x,mean=-2,sd=0.5)+0.5*dnorm(x,mean=1,sd=1)+0.3*dnorm(x,mean=6,sd=2)
g <- dnorm(x,mean=1,sd=5)
g <- 1.1*max(f)/max(g)*g
xx <- c(6.1,2.7)
yy <- c(0.16,0.12)
plot(x,g,type='l',col='red',xlab='x',ylab="pdf",ylim=c(0,1.05*max(g)),bty='l')
lines(x,f,col='black')
text(xx,yy,labels=c("M g","f"),col=c("red","black"),font=3,cex=1.5)
par(op)
```    

-----------------------------------

First, let's suppose that we wish to simulate $X\!\sim\!\mathrm{uniform}(D)$, where ${D}\subset{U}$.
If we know how to generate $Y\!\sim\!\mathrm{uniform}(U)$, then we can simply do so until ${Y}\in{D}$, at which point we take $X=Y$.
Since for any $A\subset{D}$, 
$$\mathrm{Prob}[X\in{A]} = \mathrm{Prob}[Y\in{A]|Y\in{D}} = \frac{\mathrm{area}(A)}{\mathrm{area}(U)}\Big{/}\frac{\mathrm{area}(D)}{\mathrm{area}(U)} = \frac{\mathrm{area}(A)}{\mathrm{area}(D)},$$
it follows that $Y\!\sim\!\mathrm{uniform}(D)$.
This is akin to throwing darts.
If the darts are thrown in such a way as to be equally likely to land anywhere in $U$, then those that do land in $D$ are equally likely to land anywhere in $D$.

There is another, seemingly trivial fact, that underlies the rejection method and that @Robert2004 refer to as the *fundamental theorem of simulation*.
Let $h$ be an arbitary positive, integrable function, let $D=\{(x,u): 0{\le}u{\le}h(x)\}$ be the area under the graph of $h$, and consider the random pair $(X,U)\!\sim\!\mathrm{uniform}(D)$.
What is the marginal distribution of $X$?
$$\int_0^{h(x)}\!\mathrm{d}u = h(x)$$

So $h$ is the probability distribution function for $X$!

This suggests the following *rejection method* for simulating an arbitrary random variable.
Let $f$ be the target distribution and $g$ be another distribution function (from which it is easy to simulate) (see Figure above).
Let $M$ be such that $M\,g(x){\ge}f(x)$ for all $x$.
The following procedure simulates $X\!\sim\!f$.

1. draw $Y\!\sim\!g$ and $U\!\sim\!\mathrm{uniform}(0,M\,g(Y))$.
2. if $U{\le}f(Y)$, then let $X=Y$ else repeat step 1.


## Monte Carlo integration

Monte Carlo integration is a technique developed for evaluating integrals.
First let's look at the example of finding the area of a region, $D\subset\mathbb{R}^n$, with complicated boundaries.
Solution: find some region $U{\supset}D$ and throw darts at $U$.
Count up the darts that land in $D$ and reckon the area of $D$ relative to $U$ as the fraction of darts that land in $D$.
Let $X_k$, $k=1,2,\dots,N$ be $N$ random darts thrown at $U$.
$$\frac{\mathrm{area}(D)}{\mathrm{area}(U)} 
= \frac{\int_{D}\!\mathrm{d}x}{\int_{U}\!\mathrm{d}x} 
= \frac{\int_{U}\!I_D(x)\,\mathrm{d}x}{\int_{U}\!\mathrm{d}x} 
\approx \frac{\sum_{k=1}^{N}\!I_{D}(X_k)}{\sum_{k=1}^{N}\!1} 
= \frac{1}{N}\,\sum_{k=1}^{N}\!I_{D}(X_k)$$

In the above, the *indicator function* $I_D$ is defined by
$$I_D(x) = \begin{cases} 1 &x\in{D}\\ 0 &x\notin{D}\end{cases}$$

We've just shown that we can evaluate the integral of $I_D$ over $U$.
More generally, we can use this technique to approximate the integral of any integrable function, $h$, on $U$.
$$\frac{\int_{U}\!h(x)\,\mathrm{d}x}{\int_{U}\!\mathrm{d}x} \approx \frac{1}{N}\,\sum_{k=1}^{N}\!h(X_k)$$

Here, we're relying on the assumption that the darts fall uniformly on $U$.
More generally still, we can compute an expectation with respect to an arbitrary distribution using Monte Carlo.
This leads to what is known as the *fundamental theorem of Monte Carlo integration*.
Specifically, let $f(x)$ be the probability distribution function for $x$ and let $X_k$, $k=1,\dots,N$ be independent and distributed according to $f$.
We speak of $X_k$ as random samples from $f$.
Let $\overline{h_N}$ be the empirical average of $h$:
$$\overline{h_N} = \frac{1}{N}\,\sum_{k=1}^{N}\!h(X_k).$$
Then $\overline{h_N}$ converges to $\mathbb{E}[h(X)]$ as $N\to\infty$ with probability 1.
Thus
$$\overline{h_N} = \frac{1}{N}\,\sum_{k=1}^{N}\!h(X_k) \approx \mathbb{E}[h(X)] = \int\!h(x)\,f(x)\,\mathrm{d}x.$$

Moreover, we can estimate the error in this approximation, because the empirical variance
$$v_N = \frac{1}{N-1}\,\sum_{k=1}^{N}\!\left(h(X_k)-\overline{h_N}\right)^2$$
approximates the true variance, $\mathrm{Var}[h(X)]=\mathbb{E}[(h(X)-\mathbb{E}[h(X)])^2]$.
This implies that the standard error on the approximation $\overline{h_N}=\mathbb{E}[h(X)]$ is 
$$\sqrt{\frac{v_N}{N}}$$
and that the error is approximately normally distributed:
$$\overline{h_N}-\mathbb{E}[h(X)]\;\sim\;\mathrm{normal}\left(0,\sqrt{\frac{v_N}{N}}\right).$$

## Importance sampling

Sometimes it is difficult to sample directly from the distribution of $X$.
In this case, we can often make use of *importance sampling*, in which we generate random samples from another distribution (easier to simulate) and make the appropriate correction.

Specifically, suppose we wish to compute $\mathbb{E}[h(X)]$, where $X\sim{f}$, but that it's difficult or impossible to draw random samples from $f$.
Suppose $g$ is a probability distribution from which it's relatively easy to draw samples and let $Y_k\sim{g}$ for $k=1\,\dots,N$ be i.i.d. random variables drawn from $g$.
Notice that
$$\mathbb{E}[h(X)] = \int\!h(x)\,f(x)\,\mathrm{d}x = \int\!h(x)\,\frac{f(x)}{g(x)}\,g(x)\,\mathrm{d}x.$$
So we can use the Monte Carlo integration theorem to conclude that
$$\mathbb{E}[h(X)] \approx \frac{1}{N}\,\sum_{k=1}^{N}\!h(Y_k)\,\frac{f(Y_k)}{g(Y_k)}.$$

Since $\mathbb{E}[f(Y)/g(Y)]=1$, we can replace the importance sampling rule above with another, which is slightly less accurate but more precise.
Let $w_k=f(Y_k)/g(Y_k)$ be the *importance weights*.
Then we have
$$\mathbb{E}[h(X)] \approx \overline{h} = \frac{\sum\!w_k\,h(Y_k)}{\sum\!w_k}.$$
The Monte Carlo variance associated with this estimate is
$$\frac{\sum\!w_k\,(h(Y_k)-\overline{h})^2}{\sum\!w_k}.$$

Obtaining accurate estimates requires some thought to the importance distribution $g$.
Specifically, if the tails of $g$ are lighter than those of $f$, the Monte Carlo variance will be inflated and the estimates can be unusable.

## References
