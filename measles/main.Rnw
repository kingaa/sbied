\input{../_includes/header}

\newcommand\CHAPTER{5}
\title{Lesson \CHAPTER.\\Case study:\\Measles in large and small towns}
\author{A. A. King, E. L. Ionides, Q.-Y. Lin, K. Asfaw}

<<knitr_opts,include=FALSE,cache=FALSE,purl=FALSE>>=
source("../_includes/setup.R", local = knitr::knit_global())
@

<<read_chunks,include=FALSE,cache=FALSE,purl=FALSE>>=
read_chunk("codes.R")
knitr::opts_chunk$set(purl=FALSE)
@

<<plot_colors,include=FALSE>>=
line.color <- "red"
plot.color <- "black"
@

\begin{document}

<<prelims,echo=FALSE,cache=FALSE>>=
@

\maketitle

\mode<article>{\tableofcontents}

\mode<presentation>{
  \begin{frame}{Outline}
    \tableofcontents
  \end{frame}
}

\section{Introduction}

\begin{frame}{Objectives}
  \begin{itemize}
  \item To display a published case study using plug-and-play methods with non-trivial model complexities.
  \item To show how extra-demographic stochasticity can be modeled.
  \item To demonstrate the use of covariates in \package{pomp}.
  \item To demonstrate the use of profile likelihood in scientific inference.
  \item To discuss the interpretation of parameter estimates.
  \item To emphasize the potential need for extra sources of stochasticity in modeling.
  \end{itemize}
\end{frame}

\begin{frame}[fragile,allowframebreaks=0.8]{Challenges in inference from disease dynamics}
  \begin{itemize}
  \item Understanding, forecasting, managing epidemiological systems increasingly depends on models.
  \item Dynamic models can be used to test causal hypotheses.
  \item Real epidemiological systems:
    \begin{itemize}
    \item are nonlinear
    \item are stochastic
    \item are nonstationary
    \item evolve in continuous time
    \item have hidden variables
    \item can be measured only with (large) error
    \end{itemize}
  \item Dynamics of infectious disease outbreaks illustrate this well.
  \item Measles is the paradigm for a nonlinear ecological system that can be well described by low-dimensional nonlinear dynamics.
  \item A tradition of careful modeling studies have proposed and found evidence for a number of specific mechanisms, including
    \begin{itemize}
    \item a high value of $R_0$ (c. 15--20)
    \item under-reporting
    \item seasonality in transmission rates associated with school terms
    \item response to changing birth rates
    \item a birth-cohort effect
    \item metapopulation dynamics
    \item fadeouts and reintroductions that scale with city size
    \item spatial traveling waves
    \end{itemize}
  \item Much of this evidence has been amassed from fitting models to data, using a variety of methods.
  \item See \citet{Rohani2010} for a review of some of the high points.
  \end{itemize}
  <<load-data,echo=FALSE>>=
  @
  <<some-data-plot,echo=FALSE,eval=FALSE>>=
  measles %>%
    subset(town %in% c("London","Liverpool","Hastings")) %>%
    ggplot(aes(x=date,y=cases))+
    geom_line()+
    facet_grid(town~.,scales='free_y')+
    labs(y="weekly cases")
  @
\end{frame}

\AtBeginSection[]{
  \begin{frame}<beamer>
    \frametitle{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}

\section{Model and implementation}

\subsection{Overview}

\begin{frame}[allowframebreaks]{Measles in England and Wales}
  \begin{itemize}
  \item We revisit a classic measles data set, weekly case reports in 954 urban centers in England and Wales during the pre-vaccine era (1950--1963).
  \item We examine questions regarding:
  \begin{itemize}
    \item measles extinction and recolonization
    \item transmission rates
    \item seasonality
    \item resupply of susceptibles
  \end{itemize}
  \item We use a model that
    \begin{enumerate}
    \item expresses our current understanding of measles dynamics
    \item includes a long list of mechanisms that have been proposed and demonstrated in the literature
    \item cannot be fit by existing likelihood-based methods
    \end{enumerate}
  \item We examine data from large and small towns using the same model, something no existing methods have been able to do.
  \item We ask: does our perspective on this disease change when we expect the models to explain the data in detail?
  \item What bigger lessons can we learn regarding inference for dynamical systems?
  \end{itemize}
\end{frame}

\subsection{Data sets}

\begin{frame}[allowframebreaks]{Data sets}
  \begin{itemize}
  \item He, Ionides, \& King, \emph{J. R. Soc. Interface} (2010)
  \item Twenty towns, including
    \begin{itemize}
    \item 10 largest
    \item 10 smaller, chosen at random
    \end{itemize}
  \item Population sizes: 2k--3.4M
  \item Weekly case reports, 1950--1963
  \item Annual birth records and population sizes, 1944--1963
  \end{itemize}
\end{frame}

\begin{frame}{Map of cities in the analysis}
  <<map,echo=FALSE>>=
  read_csv("data/GB_Coast.csv") %>%
    rename(long=Long,lat=Lat) -> coast

  ggplot(mapping=aes(x=long,y=lat))+
    geom_polygon(data=coast,fill=NA,color="blue")+
    geom_point(data=coord,color='red',alpha=1)+
    coord_map(projection="lambert",parameters=c(-2,53))+
    labs(x="",y="")+
    theme_void()
  @
\end{frame}

\begin{frame}{City case counts I: smallest 8 cities}
  <<dataplot,echo=FALSE,fig.height=4>>=
  demog %>%
    group_by(town) %>%
    summarize(mean.pop=mean(pop)) %>%
    ungroup() %>%
    arrange(mean.pop) -> meanpop

  measles %>%
    mutate(town=ordered(town,levels=meanpop$town)) %>%
    filter(town %in% meanpop$town[1:8]) %>%
    ggplot(aes(x=date,y=cases))+
    geom_line()+
    scale_y_continuous(breaks=c(0,4,40,400,4000),trans=scales::log1p_trans())+
    facet_wrap(~town,ncol=2)+theme(text=element_text(size=7))
  @
\end{frame}

\begin{frame}{City case counts II: largest 8 cities}
  <<dataplot2,echo=FALSE,fig.height=4>>=
  measles %>%
    mutate(town=ordered(town,levels=meanpop$town)) %>%
    filter(town %in% meanpop$town[13:20]) %>%
    ggplot(aes(x=date,y=cases))+
    geom_line()+
    scale_y_continuous(breaks=c(0,4,40,400,4000),trans=scales::log1p_trans())+
    facet_wrap(~town,ncol=2)+theme(text=element_text(size=7))
  @
\end{frame}

\subsection{Modeling}

\begin{frame}{Continuous-time Markov process model}
  <<seir-diagram,echo=FALSE,cache=FALSE,eval=FALSE>>=
  library(DiagrammeR)
  DiagrammeR("digraph SEIR {
  graph [rankdir=TD, overlap=false, fontsize = 10]
  node[shape=egg, label='B'] b;
  subgraph {
    rank=same;
    node[shape=oval, label='S'] S;
    node[shape=oval, label='E'] E;
    node[shape=oval, label='I'] I;
    node[shape=oval, label='R'] R;
    S->E E->I I->R
  }
  node[shape=diamond, label='dead'] d;
  b->S
  {S E I R}->d
   }",type="grViz",engine="dot",height=300,width=800)
  @
  \begin{center}
    \includegraphics[width=0.5\linewidth]{./model_diagram.png}
  \end{center}
\end{frame}

\begin{frame}[allowframebreaks]{Continuous-time Markov process model}
  \begin{itemize}
  \item Covariates:
    \begin{itemize}
    \item $B(t) = \text{birth rate, from data}$
    \item $N(t) = \text{population size, from data}$
    \end{itemize}

  \item Entry into susceptible class:
    $$\mu_{BS}(t) = (1-c)\,B(t-\tau)+c\,\delta(t-\lfloor t\rfloor)\,\int_{t-1}^{t}\,B(t-\tau-s)\,ds$$
    \begin{itemize}
    \item $c = \text{cohort effect}$
    \item $\tau = \text{school-entry delay}$
    \item $\lfloor t \rfloor = \text{most recent 1 September before}\ t$
    \end{itemize}
  \item Force of infection:
    $$\mu_{SE}(t) = \tfrac{\beta(t)}{N(t)}\,(I+\iota)^{\alpha}\,\zeta(t)$$
    \begin{itemize}
    \item $\iota = \text{imported infections}$
    \item $\zeta(t) = \text{Gamma white noise with intensity}\,\sigma_{SE}$ \citep{He2010,Bhadra2011}
    \item school-term transmission:
      $$\beta(t) = \begin{cases}\beta_0\,\big(1+a(1-p)/p\big) &\text{during term}\\\beta_0\,(1-a) &\text{during vacation}\end{cases}$$
      \begin{itemize}
      \item $a= \text{amplitude of seasonality}$
      \item $p=0.7589$ is the fraction of the year children are in school.
      \item The factor $(1-p)/p$ ensures that the average transmission rate is $\beta_0$.
      \end{itemize}
    \end{itemize}
  \item Overdispersed binomial measurement model: $\mathrm{cases}_t\,\vert\,\Delta{N}_{IR}=z_t \sim \dist{Normal}{\rho\,z_t,\rho\,(1-\rho)\,z_t+(\psi\,\rho\,z_t)^2}$
  \end{itemize}
\end{frame}

\subsection{Model implementation in \package{pomp}}

\begin{frame}[fragile]{Implementation in \package{pomp}}
  \begin{itemize}
  \item Codes that implement the model in \package{pomp} are to be found on \link{./codes.R}{the course website}.
  \item In those codes, we first load the needed packages and set the random seed, to allow reproducibility.
  \item Note that the codes make heavy use of the \package{ggplot2} plotting package and \package{tidyverse} methods.
  \item In particular, we use the convenient \package{magrittr} pipe syntax, which is explained \link{https://kingaa.github.io/R_Tutorial/munging.html\#the-magrittr-syntax}{elsewhere}.
  \end{itemize}
\end{frame}

\begin{frame}[fragile,allowframebreaks]{Data and covariates}
  \begin{itemize}
  \item The data are measles reports from 20 cities in England and Wales.
  \item We also have information on the population sizes and birth-rates in these cities;
    we'll treat these variables as covariates.
  \item The codes illustrate the pre-processing of the measles and demography data using London as an example.
  \end{itemize}

  \framebreak

  Measles case reports from London:
  
  <<london-data,include=FALSE>>=
  @
  <<data-plot,fig.height=3,out.width="0.8\\textwidth",echo=FALSE>>=
  dat %>% ggplot(aes(x=time,y=cases))+geom_line()
  @

  \framebreak

  We smooth the covariates and delay the entry of newborns into the susceptible pool.
  <<prep-covariates,include=FALSE>>=
  @
  <<covarplot,out.width="0.8\\textwidth",echo=FALSE>>=
  library(cowplot)
  demogLondon %>%
    select(year,pop) %>%
    ggplot(aes(x=year,y=pop))+
    geom_point()+
    geom_line(
      data=covar %>% select(year=time,pop),
      color='black'
    )-> pl1

  demogLondon %>%
    ggplot(aes(x=year,y=births))+
    geom_point()+
    geom_line(
      data=covar1 %>% select(time,births=birthrate1),
      mapping=aes(x=time-0.5,y=births),
      color='black'
    )+
    geom_line(
      data=covar1 %>% select(time,births=birthrate),
      mapping=aes(x=time-0.5,y=births),
      color='red'
    )-> pl2

  plot_grid(pl1,pl2,ncol=1,align="v")
  @
\end{frame}


\begin{frame}{The partially observed Markov process model}

  We require a simulator for our model.
  Notable complexities include:
  \begin{enumerate}
  \item Incorporation of the known birthrate.
  \item The birth-cohort effect: a specified fraction (\code{cohort}) of the cohort enter the susceptible pool all at once.
  \item Seasonality in the transmission rate: during school terms, the transmission rate is higher than it is during holidays.
  \item Extra-demographic stochasticity in the form of a Gamma white-noise term acting multiplicatively on the force of infection.
  \item Demographic stochasticity implemented using Euler-multinomial distributions.
  \end{enumerate}
\end{frame}

<<rproc,include=FALSE>>=
@

\begin{frame}[fragile,allowframebreaks]{Implementation of the process model}

  Let's walk through the rprocess C snippet.
  
<<rproc_listing1,echo=FALSE>>=
library(stringi)
txt <- stri_split_lines1(rproc@text)
cat(txt[2:9],sep="\n")
@

\framebreak

<<rproc_listing1a,echo=FALSE>>=
cat(txt[11:19],sep="\n")
@

\framebreak

<<rproc_listing2,echo=FALSE>>=
cat(txt[21:28],sep="\n")
@ 

\framebreak

<<rproc_listing3,echo=FALSE>>=
cat(txt[30:35],sep="\n")
@ 

\framebreak

<<rproc_listing4,echo=FALSE>>=
cat(txt[37:43],sep="\n")
@ 

\framebreak

<<rproc_listing5,echo=FALSE>>=
cat(txt[45:50],sep="\n")
@ 
\end{frame}

\begin{frame}[fragile]{Process model observations}
  \begin{itemize}
  \item In the above, \code{C} represents the true incidence, i.e., the number of new infections occurring over an interval.
  \item Since recognized measles infections are quarantined, we argue that most infection occurs before case recognition so that true incidence is a measure of the number of individuals progressing from the I to the R compartment in a given interval.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{State initializations}
  We complete the process model definition by specifying the distribution of initial unobserved states.
  The following codes assume that the fraction of the population in each of the four compartments is known.
  <<rinit,include=FALSE>>=
  @
  <<rinit_listing,echo=FALSE>>=
  cat(rinit@text)
  @ 
\end{frame}

\begin{frame}[fragile,allowframebreaks]{Measurement model}
  \begin{itemize}
  \item We'll model both under-reporting and measurement error.
  \item We want $\mathbb{E}[\mathrm{cases}|C] = \rho\,C$, where $C$ is the true incidence and $0<\rho<1$ is the reporting efficiency.
  \item We'll also assume that $\mathrm{Var}[\mathrm{cases}|C] = \rho\,(1-\rho)\,C + (\psi\,\rho\,C)^2$, where $\psi$ quantifies overdispersion.
  \item Note that when $\psi=0$, the variance-mean relation is that of the binomial distribution.
    To be specific, we'll choose
    $\mathrm{cases}\;\vert\;C \sim f(\cdot\;\vert\;\rho,\psi,C)$, where
    \begin{equation*}
      \begin{split}
        f(c\;\vert\;\rho,\psi,C) = &\Phi(c+\tfrac{1}{2},\rho\,C,\rho\,(1-\rho)\,C+(\psi\,\rho\,C)^2)\\
        &-\Phi(c-\tfrac{1}{2},\rho\,C,\rho\,(1-\rho)\,C+(\psi\,\rho\,C)^2).
      \end{split}
    \end{equation*}
    Here, $\Phi(x,\mu,\sigma^2)$ is the c.d.f. of the normal distribution with mean $\mu$ and variance $\sigma^2$.
  \end{itemize}

  \framebreak

  The following computes $\mathbb{P}[\mathrm{cases}|C]$.
  <<dmeasure,include=FALSE>>=
  @
  <<dmeas_listing,echo=FALSE>>=
  cat(dmeas@text)
  @ 

  \framebreak
  
  The following codes simulate $\mathrm{cases} | C$.
  <<rmeasure,include=FALSE>>=
  @
  <<rmeas_listing,echo=FALSE>>=
  cat(rmeas@text)
  @ 
\end{frame}

\begin{frame}[fragile]{Constructing the pomp object}
  <<pomp-construction>>=
  @
\end{frame}

\section{Estimation}

\subsection{\citet{He2010}}

\begin{frame}[fragile]{Estimates from \citet{He2010}}
  \citet{He2010} estimated the parameters of this model.
  The full set of estimates is included in the \Rlanguage code accompanying this document, where they are read into a data frame called \code{mles}.
  <<mles,include=FALSE>>=
  @
  
  <<mle,include=FALSE>>=
  @
  
  We verify that we get the same likelihood as \citet{He2010}.

  <<pfilter1a,eval=FALSE>>=
  @
  <<pfilter1_eval,eval=TRUE,include=FALSE,cache=FALSE>>=
  bake("pfilter1.rds",{
    <<pfilter1a>>
  }) -> pfs
  @ 
  <<pfilter1b,eval=TRUE,size="small">>=
  @
\end{frame}

\subsection{Simulations}

\begin{frame}[fragile]{Simulations at the MLE}
  <<sims1,fig.height=2,echo=TRUE>>=
  m1 %>%
    simulate(params=theta,nsim=3,format="d",include.data=TRUE) %>%
    ggplot(aes(x=time,y=cases,group=.id,color=(.id=="data")))+
    guides(color="none")+
    geom_line()+facet_wrap(~.id,ncol=2)
  @
\end{frame}

\subsection{Parameter estimation}

\begin{frame}[fragile]{Parameter transformations}
  \begin{itemize}
  \item The parameters are constrained to be positive, and some of them are constrained to lie between $0$ and $1$.

  \item We can turn the likelihood maximization problem into an unconstrained maximization problem by transforming the parameters.

  \item Specifically, to enforce positivity, we log transform,
    to constrain parameters to $(0,1)$, we logit transform,
    and to confine parameters to the unit simplex, we use the log-barycentric transformation.
  \end{itemize}

  <<transforms>>=
  @
  <<fold-transforms,include=FALSE,purl=TRUE>>=
  @ 

\end{frame}

\section{Findings}

\begin{frame}[fragile]{Results from \citet{He2010}}
  The \link{./profile.html}{linked document} shows how a likelihood profile can be constructed using IF2.
  The fitting procedure used is as follows:
  \begin{itemize}
  \item A large number of searches were started at points across the parameter space.
  \item Iterated filtering was used to maximize the likelihood.
  \item We obtained point estimates of all parameters for 20 cities.
  \item We constructed profile likelihoods to quantify uncertainty in London and Hastings.
  \end{itemize}
\end{frame}

\subsection{Notable findings}

\begin{frame}[fragile]{Imported infections}
  $$\text{force of infection} = \mu_{SE}=\frac{\beta(t)}{N(t)}\,(I+\iota)^{\alpha}\,\zeta(t)$$
  <<imports,results="hide",echo=FALSE,out.width="4in">>=
  best <- read.csv('data/iota.csv',row.names=1)

  op <- par(
    font=2,
    fig=c(0,1,0,1),
    mar=c(4,4,4,4),
    bty='l'
  )
  plot.new()
  mtext(side=2,line=2.5,text="profile log likelihood",adj=0.5)
  mtext(side=1,line=2,text=expression(paste("imported infections, ",iota)),adj=0.5)

  x <- best[grep('London',rownames(best)),]
  fit <- loess(loglik~log(iota),data=x,span=0.7)
  nd <- data.frame(iota=with(x,exp(seq(from=min(log(iota)),to=max(log(iota)),length=100))))
  nd$loglik <- predict(fit,newdata=nd)
  cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
  conf.int <- range(nd$iota[nd$loglik>cutoff],na.rm=T)
  par(fig=c(0,1,0.5,1),mar=c(0.5,4,4,4),new=T)
  plot(
    loglik~iota,
    data=x,
    font=2,
    bty='l',
    ann=FALSE,
    xaxt='n',
    log='x',
    xlim=c(0.001,100),
    ylim=max(x$loglik)+c(-15,1)
  )
  axis(side=1,at=c(0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100),labels=F)
  lines(loglik~iota,data=nd,col=line.color)
  abline(h=cutoff,lty='33')
  abline(v=conf.int,lty='63')
  text(0.002,cutoff,"London",pos=3)

  x <- best[grep('Hastings',rownames(best)),]
  fit <- loess(loglik~log(iota),data=x,span=0.7)
  nd <- data.frame(iota=with(x,seq(from=min(iota),to=max(iota),length=100)))
  nd$loglik <- predict(fit,newdata=nd)
  cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
  conf.int <- range(nd$iota[nd$loglik>cutoff],na.rm=T)
  par(fig=c(0,1,0,0.5),mar=c(4,4,0.5,4),new=T)
  plot(
    loglik~iota,
    data=x,
    font=2,
    bty='l',
    ann=FALSE,
    xaxt='n',
    log='x',
    xlim=c(0.001,100),
    ylim=max(x$loglik)+c(-15,1)
  )
  axis(
    side=1,
    at=c(0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100),
    labels=c(expression(10^-3),"",expression(10^-2),"",expression(10^-1),"",expression(10^0),"",expression(10^1),"",expression(10^2))
  )
  lines(loglik~iota,data=nd,col=line.color)
  abline(h=cutoff,lty='33')
  abline(v=conf.int,lty='63')
  text(0.002,cutoff,"Hastings",pos=3)
  par(op)
  @
\end{frame}

\begin{frame}[fragile]{Seasonality}
  <<amplitude,results="hide",echo=FALSE>>=
  best <- read.csv('data/amplitude.csv',row.names=1)

  op <- par(
    font=2,
    fig=c(0,1,0,1),
    mar=c(4,4,4,4),
    bty='l'
  )
  plot.new()
  mtext(side=2,line=2.5,text="profile log likelihood",adj=0.5)
  mtext(side=1,line=2,text="amplitude of term-time seasonality",adj=0.5)

  x <- best[grep('London',rownames(best)),]
  fit <- loess(loglik~amplitude,data=x,span=0.7)
  nd <- data.frame(amplitude=with(x,seq(from=min(amplitude),to=max(amplitude),length=100)))
  nd$loglik <- predict(fit,newdata=nd)
  cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
  conf.int <- range(nd$amplitude[nd$loglik>cutoff],na.rm=T)
  par(fig=c(0,1,0.5,1),mar=c(0.5,4,4,4),new=T)
  plot(
    loglik~amplitude,
    data=x,
    font=2,
    bty='l',
    ann=FALSE,
    xaxt='n',
    xlim=c(0,1),
    ylim=max(x$loglik)+c(-10,1)
  )
  axis(side=1,at=seq(0,1,by=0.2),labels=F)
  lines(loglik~amplitude,data=nd,col=line.color)
  abline(h=cutoff,lty='33')
  abline(v=conf.int,lty='63')
  text(0.9,cutoff,"London",pos=3)

  x <- best[grep('Hastings',rownames(best)),]
  fit <- loess(loglik~amplitude,data=x,span=0.7)
  nd <- data.frame(amplitude=with(x,seq(from=min(amplitude),to=max(amplitude),length=100)))
  nd$loglik <- predict(fit,newdata=nd)
  cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
  conf.int <- range(nd$amplitude[nd$loglik>cutoff],na.rm=T)
  par(fig=c(0,1,0,0.5),mar=c(4,4,0.5,4),new=T)
  plot(
    loglik~amplitude,
    data=x,
    font=2,
    bty='l',
    ann=FALSE,
    xaxt='n',
    xlim=c(0,1),
    ylim=max(x$loglik)+c(-10,1)
  )
  axis(side=1,at=seq(0,1,by=0.2))
  lines(loglik~amplitude,data=nd,col=line.color)
  abline(h=cutoff,lty='33')
  abline(v=conf.int,lty='63')
  text(0.9,cutoff,"Hastings",pos=3)
  par(op)
  @
\end{frame}

\begin{frame}[fragile]{Cohort effect}
  <<cohort-effect,echo=FALSE,results="hide">>=
  best <- read.csv('data/cohort.csv',row.names=1)

  op <- par(
    font=2,
    fig=c(0,1,0,1),
    mar=c(4,4,4,4),
    bty='l'
  )
  plot.new()
  mtext(side=2,line=2.5,text="profile log likelihood",adj=0.5)
  mtext(side=1,line=2,text="cohort entry fraction",adj=0.5)

  x <- best[grep('London',rownames(best)),]
  fit <- loess(loglik~cohort,data=x,span=0.7)
  nd <- data.frame(cohort=with(x,seq(from=min(cohort),to=max(cohort),length=100)))
  nd$loglik <- predict(fit,newdata=nd)
  cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
  conf.int <- range(nd$cohort[nd$loglik>cutoff],na.rm=T)
  par(fig=c(0,1,0.5,1),mar=c(0.5,4,4,4),new=T)
  plot(
    loglik~cohort,
    data=x,
    font=2,
    bty='l',
    ann=FALSE,
    xaxt='n',
    xlim=c(0,1),
    ylim=max(x$loglik)+c(-10,1)
  )
  axis(side=1,at=seq(0,1,by=0.2),labels=F)
  lines(loglik~cohort,data=nd,col=line.color)
  abline(h=cutoff,lty='33')
  abline(v=conf.int,lty='63')
  text(0.1,cutoff,"London",pos=3)

  x <- best[grep('Hastings',rownames(best)),]
  fit <- loess(loglik~cohort,data=x,span=0.7)
  nd <- data.frame(cohort=with(x,seq(from=min(cohort),to=max(cohort),length=100)))
  nd$loglik <- predict(fit,newdata=nd)
  cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
  conf.int <- range(nd$cohort[nd$loglik>cutoff],na.rm=T)
  par(fig=c(0,1,0,0.5),mar=c(4,4,0.5,4),new=T)
  plot(
    loglik~cohort,
    data=x,
    font=2,
    bty='l',
    ann=FALSE,
    xaxt='n',
    xlim=c(0,1),
    ylim=max(x$loglik)+c(-10,1)
  )
  axis(side=1,at=seq(0,1,by=0.2))
  lines(loglik~cohort,data=nd,col=line.color)
  abline(h=cutoff,lty='33')
  abline(v=conf.int,lty='63')
  text(0.1,cutoff,"Hastings",pos=1)
  par(op)
  @
\end{frame}

\begin{frame}[fragile]{Birth delay}
  <<delay,eval=TRUE,echo=FALSE,results="hide",fig.height=5,out.width="0.7\\textwidth">>=
  x <- read.csv('data/delay.csv',row.names=1)
  plot(
    x$delay,
    x$loglik,
    type='n',
    xlab=expression(paste(tau,' (yr)')),
    ylab='profile log likelihood'
  )
  points(x$delay,x$loglik,col='black')
  lines(x$delay,x$loglik,col='red')
  abline(h=max(x$loglik)-0.5*qchisq(p=c(0.95,0.99),df=1),lty='33')
  @

  Profile likelihood for birth-cohort delay, showing 95\% and 99\% critical values of the log likelihood.
\end{frame}

\begin{frame}[fragile]{Reporting rate}
  <<report-rate,echo=FALSE,results="hide">>=
  mles %>% select(town,rho) -> est.rho

  demog %>%
    subset(year==1950) %>%
    select(town,pop) -> pop

  measles %>%
    mutate(year=as.integer(format(date,"%Y"))) %>%
    group_by(town,year) %>%
    summarize(cases=sum(cases)) %>%
    ungroup() %>%
    left_join(demog,by=c("town","year")) %>%
    group_by(town) %>%
    transmute(
      cases=cumsum(cases),
      births=cumsum(births)
    ) %>%
    ungroup() -> m

  m %>%
    group_by(town) %>%
    do({
      fit <- lm(cases~births,data=.)
      data.frame(slope=fit$coefficients[2])
    }) %>%
    left_join(pop,by='town') %>%
    left_join(est.rho,by='town') %>%
    rename(regression=slope,model=rho) %>%
    gather(variable,value,-town,-pop) %>%
    ggplot(aes(x=town,y=value,shape=variable,group=town))+
    geom_point(size=3)+
    geom_line(alpha=0.5)+
    labs(x="",y="estimated reporting rate",
         variable="estimate")+
    theme(axis.text.x=element_text(angle=90,hjust=1))
  @
\end{frame}


\begin{frame}[fragile]{Predicted vs observed critical community size}
  <<fadeouts,echo=FALSE,results="hide">>=
  op <- par(
    font=2,
    fig=c(0,1,0,1),
    mar=c(4,4,4,4),
    bty='l'
  )
  ccssim <- read.csv('data/ccssim.csv',comment.char='#')
  ccsdata <- read.csv('data/fadeouts_954.csv',row.names=1,comment.char='#')
  plot(
    prop.fadeout~mean.pop,
    data=ccsdata,
    bty='l',
    log='x',
    ann=FALSE,
    pch=20,
    xlim=range(ccssim$pop),
    ylim=c(0,1)
  )
  lines(prop.fadeout~pop,data=ccssim,lwd=3,col=line.color)
  mtext(side=1,line=3,text='community size')
  mtext(side=2,line=3,text='proportion of weeks without cases')
  par(op)
  @
\end{frame}

\subsection{Problematic results}

\begin{frame}[fragile]{$R_0$ estimates inconsistent with literature}
  \begin{itemize}
  \item Recall that $R_0$ : a measure of how communicable an infection is.
  \item Existing estimates of $R_0$ (c. 15--20) come from two sources: serology surveys, and models fit to data using feature-based methods.
  \end{itemize}
  <<R0,echo=FALSE,results="hide",fig.height=3>>=
  best <- read.csv('data/R0profile.csv',row.names=1)

  op <- par(
    font=2,
    fig=c(0,1,0,1),
    mar=c(4,4,4,4),
    bty='l'
  )
  plot.new()
  mtext(side=2,line=2.5,text="profile log likelihood",adj=0.5)
  mtext(side=1,line=2,text=expression(R[0]),adj=0.5)

  x <- best[grep('London',rownames(best)),]
  fit <- loess(loglik~R0,data=x,span=0.7)
  nd <- data.frame(R0=with(x,seq(from=min(R0),to=max(R0),length=100)))
  nd$loglik <- predict(fit,newdata=nd)
  cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
  conf.int <- range(nd$R0[nd$loglik>cutoff],na.rm=T)
  par(fig=c(0,1,0.5,1),mar=c(0.5,4,4,4),new=T)
  plot(
    loglik~R0,
    data=x,
    font=2,
    bty='l',
    ann=FALSE,
    xaxt='n',
    xlim=c(10,100),
    ylim=max(x$loglik)+c(-10,1)
  )
  axis(side=1,at=seq(10,100,by=30),labels=F)
  lines(loglik~R0,data=nd,col=line.color)
  abline(h=cutoff,lty='33')
  abline(v=conf.int,lty='63')
  text(90,max(x$loglik),"London")

  x <- best[grep('Hastings',rownames(best)),]
  fit <- loess(loglik~R0,data=x,span=0.7)
  nd <- data.frame(R0=with(x,seq(from=min(R0),to=max(R0),length=100)))
  nd$loglik <- predict(fit,newdata=nd)
  cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
  conf.int <- range(nd$R0[nd$loglik>cutoff],na.rm=T)
  par(fig=c(0,1,0,0.5),mar=c(4,4,0.5,4),new=T)
  plot(
    loglik~R0,
    data=x,
    font=2,
    bty='l',
    ann=FALSE,
    xaxt='n',
    xlim=c(10,100),
    ylim=max(x$loglik)+c(-10,1)
  )
  axis(side=1,at=seq(10,100,by=30))
  lines(loglik~R0,data=nd,col=line.color)
  abline(h=cutoff,lty='33')
  abline(v=conf.int,lty='63')
  text(90,max(x$loglik),"Hastings")
  par(op)
  @
\end{frame}

\begin{frame}[fragile]{Parameter estimates}
  <<est-table,echo=FALSE,size="tiny">>=
  mean.euler <- function(rate,dt=1/365) dt/(1-exp(-rate*dt))

  demog %>%
    filter(year==1950) %>%
    select(town,pop) -> pop

  mles %>%
    left_join(pop,by='town') %>%
    mutate(
      IP=365*mean.euler(gamma),
      LP=365*mean.euler(sigma)
    ) %>%
    arrange(pop) %>%
    select(town,pop,R0,amplitude,LP,IP,alpha,iota,rho,psi,sigmaSE) -> est

  est %>%
    gather(variable,value,-town,-pop) %>%
    group_by(variable) %>%
    summarize(
      cor=cor(value,pop,use="all.obs",method="spearman")
    ) %>%
    spread(variable,cor) -> cors

  est %>%
    bind_rows(c(cors,town="$\\qquad r$")) %>%
    gather(var,val,-town) %>%
    mutate(val=signif(val,2)) %>%
    spread(var,val) %>%
    arrange(pop) %>%
    mutate(
      pop=coalesce(pop,1),
      town=stringi::stri_replace_all_fixed(town,"."," ")
    ) %>%
    select(
      town,
      `$N_{1950}$`=pop,`$R_0$`=R0,IP,LP,`$\\alpha$`=alpha,
      `$a$`=amplitude,`$\\iota$`=iota,`$\\psi$`=psi,`$\\rho$`=rho,
      `$\\sigma_{SE}$`=sigmaSE
    ) %>%
    column_to_rownames("town") %>%
    knitr::kable("latex",row.names=TRUE,escape=FALSE)
@

\vspace{1em}
$r=\mathrm{cor}_{S}({\cdot},{N_{1950}})$ (Spearman rank correlation)

\end{frame}

\begin{frame}[fragile]{Extrademographic stochasticity}
  $$\mu_{SE}=\frac{\beta(t)}{N(t)}\,(I+\iota)\,\zeta(t)$$
  <<env-noise,echo=FALSE,results="hide",out.width="4in">>=
  best <- read.csv('data/env_noise.csv',row.names=1)
  mean.euler <- function(rate,dt=1/365) dt/(1-exp(-rate*dt))
  best$IP <- mean.euler(best[,'gamma'])*365
  best$LP <- mean.euler(best[,'sigma'])*365
  best$sigSE <- 1.0/sqrt(best[,'eta'])

  op <- par(
    font=2,
    fig=c(0,1,0,1),
    mar=c(4,4,4,4)
  )
  plot.new()
  mtext(side=1,line=2,text=expression(sigma[SE]),adj=0.5)
  mtext(side=2,line=2.5,text="profile log likelihood",adj=0.5)
  mtext(side=4,line=2.5,text="duration (days)",adj=0.5)

  x <- best[grep('London',rownames(best)),]

  fit <- loess(loglik~sigSE,data=x,span=0.7)
  nd <- data.frame(sigSE=with(x,seq(from=min(sigSE),to=max(sigSE),length=100)))
  nd$loglik <- predict(fit,newdata=nd)
  fit <- loess(IP~sigSE,data=x,span=0.7)
  nd$IP<-predict(fit,newdata=nd)
  fit <- loess(LP~sigSE,data=x,span=0.7)
  nd$LP<-predict(fit,newdata=nd)
  cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
  ci.lond <- conf.int <- range(nd$sigSE[nd$loglik>cutoff],na.rm=T)

  par(fig=c(0,1,0.5,1),mar=c(0.5,4,4,4),new=T)
  plot(
    loglik~sigSE,
    data=x,
    bty='u',
    ann=FALSE,
    xaxt='n',
    yaxt='n',
    log='x',
    xlim=c(0.005,0.5),
    ylim=max(x$loglik)+c(-10,1)
  )
  axis(side=1,at=c(0.005,0.01,0.02,0.05,0.1,0.2,0.5),labels=F)
  axis(side=2)
  lines(loglik~sigSE,data=nd,col=line.color)
  abline(h=cutoff,lty='33')
  abline(v=conf.int,lty='63')

  IPcol='red'
  LPcol='blue'
  IPlty=1
  LPlty=1
  plot.window(xlim=c(0.005,0.5),ylim=c(0,20))
  lines(IP~sigSE,data=nd,lwd=2,col=IPcol,lty=IPlty)
  lines(LP~sigSE,data=nd,lwd=2,col=LPcol,lty=LPlty)
  axis(side=4)

  plot.window(xlim=c(1,10),ylim=c(0,1))
  text(10^(0.9),0.8,"London",pos=3)
  legend("topleft",lwd=2,col=c(IPcol,LPcol),
         lty=c(IPlty,LPlty),legend=c("IP","LP"),bty='n',bg='white')


  x <- best[grep('Hastings',rownames(best)),]

  fit <- loess(loglik~sigSE,data=x,span=0.7)
  nd <- data.frame(sigSE=with(x,seq(from=min(sigSE),to=max(sigSE),length=100)))
  nd$loglik <- predict(fit,newdata=nd)
  fit <- loess(IP~sigSE,data=x,span=0.7)
  nd$IP<-predict(fit,newdata=nd)
  fit <- loess(LP~sigSE,data=x,span=0.7)
  nd$LP<-predict(fit,newdata=nd)
  cutoff <- max(nd$loglik,na.rm=T)-0.5*qchisq(p=0.95,df=1)
  ci.hast <- conf.int <- range(nd$sigSE[nd$loglik>cutoff],na.rm=T)

  par(fig=c(0,1,0,0.5),mar=c(4,4,0.5,4),new=T)
  plot(
    loglik~sigSE,
    data=x,
    bty='l',
    ann=FALSE,
    xaxt='n',
    yaxt='n',
    log='x',
    bty='u',
    xlim=c(0.005,0.5),
    ylim=max(x$loglik)+c(-10,1)
  )
  axis(side=1,at=c(0.005,0.01,0.02,0.05,0.1,0.2,0.5))
  axis(side=2)
  lines(loglik~sigSE,data=nd,col=line.color)
  abline(h=cutoff,lty='33')
  abline(v=conf.int,lty='63')

  plot.window(xlim=c(0.005,0.5),ylim=c(0,20))
  lines(IP~sigSE,data=nd,lwd=2,col=IPcol,lty=IPlty)
  lines(LP~sigSE,data=nd,lwd=2,col=LPcol,lty=LPlty)
  axis(side=4)

  plot.window(xlim=c(1,10),ylim=c(0,1))
  text(10^(0.1),0.8,"Hastings",pos=3)

  par(op)
  @
\end{frame}

\begin{frame}[fragile]{Questions}
  \begin{itemize}
  \item What does it mean that parameter estimates from the fitting disagree with estimates from other data?
  \item How can one interpret the correlation between infectious period and city size in the parameter estimates?
  \item How do we interpret the need for extrademographic stochasticity in this model?
  \end{itemize}
\end{frame}

\begin{frame}[fragile]{Simulations at the MLE}
  <<sims2,echo=FALSE,fig.height=4>>=
  m1 %>%
    simulate(params=theta,nsim=100,format="d",include.data=TRUE) %>%
    select(time,.id,cases) -> simdat

  simdat %>%
    mutate(
      data=if_else(.id=="data","data","simulation")
    ) %>%
    group_by(time,data) %>%
    summarize(
      p=c("lo","med","hi"),
      q=quantile(cases,prob=c(0.05,0.5,0.95),names=FALSE)
    ) %>%
    ungroup() %>%
    spread(p,q) %>%
    ggplot(aes(x=time,y=med,color=data,fill=data,ymin=lo,ymax=hi))+
    scale_color_manual(values=c(data="blue",simulation="red"))+
    scale_fill_manual(values=c(data="blue",simulation="red"))+
    geom_ribbon(alpha=0.2,color=NA)+
    geom_line()+
    guides(fill="none")+
    labs(y="cases",x="",title="London") -> pl1

  simdat %>%
    filter(.id=="data" | .id <= "5") %>%
    mutate(
      data=if_else(.id=="data","data","simulation")
    ) %>%
    ggplot(aes(x=time,y=cases,group=.id,color=data,alpha=data))+
    geom_line()+
    scale_color_manual(values=c(data="blue",simulation="red"))+
    scale_alpha_manual(values=c(data=1,simulation=0.3))+
    guides(color="none",alpha="none") -> pl2

  library(cowplot)
  plot_grid(pl1,pl2,ncol=1,align="hv",axis="tblr")
  @
\end{frame}

\section{Exercises}

\begin{frame}{\myexercise. Reformulate the model}
  \begin{itemize}
  \item Modify the \citet{He2010} model to remove the cohort effect.
    Run simulations and compute likelihoods to convince yourself that the resulting codes agree with the original ones for `cohort = 0`.
  \item Now modify the transmission seasonality to use a sinusoidal form. How many parameters must you use?
    Fixing the other parameters at their MLE values, compute and visualize a profile likelihood over these parameters.
  \end{itemize}
\end{frame}

\begin{frame}{\myexercise. Extrademographic stochasticity}
  Set the extrademographic stochasticity parameter $\sigma_{SE}=0$, set $\alpha=1$, and fix $\rho$ and $\iota$ at their MLE values, then maximize the likelihood over the remaining parameters.
  \begin{itemize}
  \item
    How do your results compare with those at the MLE? Compare likelihoods but also use simulations to diagnose differences between the models.
  \end{itemize}
\end{frame}

\mode<presentation>{
  \begin{frame}[allowframebreaks=0.8]{References}
    \bibliography{../sbied}
  \end{frame}
}
\mode<article>{
  \bibliography{../sbied}
}

\begin{frame}{License, acknowledgments, and links}

  \begin{itemize}
  \item
    This lesson is prepared for the \link{https://kingaa.github.io/sbied/}{Simulation-based Inference for Epidemiological Dynamics} module at the Summer Institute in Statistics and Modeling in Infectious Diseases, \link{https://www.biostat.washington.edu/suminst/sismid}{SISMID}.

  \item
    The materials build on \link{../acknowledge.html}{previous versions of this course and related courses}.

  \item
    Licensed under the \link{https://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.
    \includegraphics[height=12pt]{../graphics/cc-by-nc}

  \item
    Produced with R version \Sexpr{getRversion()} and \package{pomp} version \Sexpr{packageVersion("pomp")}.

  \item
    Compiled on \today.

  \end{itemize}

  \link{index.html}{Back to Lesson}
  
  \link{./codes.R}{\Rlanguage codes for this lesson}

\end{frame}

\end{document}
